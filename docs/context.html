<!DOCTYPE HTML>
<html>
<head>
    <title>Wayverb - Context</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" type="image/x-icon" href="/wayverb/assets/favicon.ico" />
	<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/wayverb/assets/css/main.css" />
    <link rel="stylesheet" href="/wayverb/assets/css/font-awesome.min.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/wayverb/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/wayverb/assets/css/ie8.css" /><![endif]-->

<!-- Scripts -->
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>

<body>
<nav id="sidebar_nav">
    <a href="/wayverb/" class="title">Wayverb</a>
    <ul>
        
        
            <li>
                <a href="/wayverb/introduction.html" >
                    Introduction
                </a>
            </li>
        
            <li>
                <a href="/wayverb/context.html" class="active">
                    Context
                </a>
            </li>
        
            <li>
                <a href="/wayverb/theory.html" >
                    Theory
                </a>
            </li>
        
            <li>
                <a href="/wayverb/image_source.html" >
                    Image-source Model
                </a>
            </li>
        
            <li>
                <a href="/wayverb/ray_tracer.html" >
                    Ray tracer
                </a>
            </li>
        
            <li>
                <a href="/wayverb/waveguide.html" >
                    Waveguide
                </a>
            </li>
        
            <li>
                <a href="/wayverb/hybrid.html" >
                    Hybrid Model
                </a>
            </li>
        
            <li>
                <a href="/wayverb/microphone.html" >
                    Microphone modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/boundary.html" >
                    Boundary modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/evaluation.html" >
                    Evaluation
                </a>
            </li>
        
            <li>
                <a href="/wayverb/conclusion.html" >
                    Conclusion
                </a>
            </li>
        
    </ul>
</nav>

<div id="page_main">
    <header>
        <ul>
            <li class="nav_menu open" >
                <a href="#sidebar_nav">
                    &#9776;
                </a>
            </li>
            <li class="nav_menu close" >
                <a href="#">
                    &#9776;
                </a>
            </li>
            <li>
                <a href="/wayverb/" >
                    Wayverb
                </a>
            </li>
        </ul>
    </header>
    <div class="inner">
        <nav id="prev_next_nav">
    
    
        
    
        
            
            
            
                <a href="/wayverb/introduction.html" class="prev_page">Introduction</a>
            

            
            
            
                <a href="/wayverb/theory.html" class="next_page">Theory</a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
</nav>

        <div id="TOC">
<ul>
<li><a href="#context">Context</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#characteristics-of-room-acoustics-simulation-methods">Characteristics of Room Acoustics Simulation Methods</a><ul>
<li><a href="#geometric-methods">Geometric Methods</a></li>
<li><a href="#wave-based-methods">Wave-based Methods</a></li>
</ul></li>
<li><a href="#existing-software">Existing Software</a></li>
<li><a href="#acoustic-simulation-and-the-creative-arts">Acoustic Simulation and the Creative Arts</a></li>
<li><a href="#project-aims">Project Aims</a><ul>
<li><a href="#proposed-solution">Proposed Solution</a></li>
</ul></li>
<li><a href="#original-contributions">Original Contributions</a><ul>
<li><a href="#chosen-simulation-techniques">Chosen Simulation Techniques</a></li>
<li><a href="#chosen-technology">Chosen Technology</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</div>
<h1 id="context" class="major">Context</h1>
<h2 id="overview">Overview</h2>
<p>Room acoustics algorithms fall into two main categories: <em>geometric</em>, and <em>wave-based</em> <span class="citation" data-cites="southern_spatial_2011">[<a href="#ref-southern_spatial_2011">1</a>]</span>. Wave-based methods aim to solve the wave equation numerically, simulating the actual behaviour of sound waves within an enclosed space. Geometric methods instead make some simplifying assumptions about the behaviour of sound, which result in faster but less accurate simulations. These assumptions generally ignore all wave properties of sound, choosing to model sound as independent <em>rays</em>, <em>particles</em>, or <em>phonons</em>.</p>
<p>The modelling of waves as particles has found great success in the field of computer graphics, where <em>ray-tracing</em> is used to simulate the reflections of light in a scene. The technique works well for simulating light because of the relatively high frequencies of the modelled waves. The wavelengths of these waves - the wavelengths of the visible spectrum - will generally be many times smaller than any surface in the scene being rendered, so wave phenomena have little or no visible effect.</p>
<p>The assumption that rays and waves are interchangeable falls down somewhat when modelling sound. The wavelengths of sound in air range from 17m to 0.017m for the audible frequency range (20Hz to 20kHz), so while the simulation may be accurate at high frequencies, at low frequencies the wavelength is of the same order as the wall surfaces in the scene. Failure to take wave effects such as interference and diffraction into account at these frequencies therefore results in noticeable approximation error <span class="citation" data-cites="savioja_overview_2015">[<a href="#ref-savioja_overview_2015">2</a>]</span>.</p>
<p>In many cases, some inaccuracy is an acceptable (or even necessary) trade-off. Wave-modelling is so computationally expensive that using it to simulate a large scene over a broad spectrum might take weeks on consumer hardware. This leaves geometric methods as the only viable alternative. Though wave-modelling been studied for some time <span class="citation" data-cites="smith_physical_1992">[<a href="#ref-smith_physical_1992">3</a>]</span>, and even applied to small simulations of strings and membranes in consumer devices such as keyboards, it is only recently, as computers have become more powerful, that these techniques have been seriously considered for room acoustics simulation.</p>
<p>Given that wave-based methods are accurate, but become more expensive at higher frequencies, and that geometric methods are inexpensive, but become less accurate at lower frequencies, it is natural to combine the two models in a way that takes advantage of the desirable characteristics of each <span class="citation" data-cites="aretz_combined_2009">[<a href="#ref-aretz_combined_2009">4</a>]</span>. That is, by using wave-modelling for low-frequency content, and geometric methods for high-frequency content, simulations may be produced which are accurate across the entire spectrum, without incurring massive computational costs.</p>
<h2 id="characteristics-of-room-acoustics-simulation-methods">Characteristics of Room Acoustics Simulation Methods</h2>
<p>A short review of acoustic simulation methods will be given here. For a more detailed survey of methods used in room acoustics, see <span class="citation" data-cites="savioja_overview_2015">[<a href="#ref-savioja_overview_2015">2</a>]</span>.</p>
<p>Figure <a href="#fig:simulation_techniques">1</a> shows the relationships between the most common simulation methods. The advantages and disadvantages of each method will be discussed throughout the remainder of this section.</p>
<figure>
<img src="images/simulation_techniques.svg" alt="Figure 1: An overview of different acoustic simulation methods, grouped by category." id="fig:simulation_techniques" /><figcaption>Figure 1: An overview of different acoustic simulation methods, grouped by category.</figcaption>
</figure>
<h3 id="geometric-methods">Geometric Methods</h3>
<p>Geometric methods can be grouped into two categories: <em>stochastic</em> and <em>deterministic</em>.</p>
<p>Stochastic methods are generally based on statistical approximation via some kind of Monte Carlo method. Such methods are approximate by nature. They aim to randomly and repeatedly sample the problem space, recording samples which fulfil some correctness criteria, and discarding the rest. By combining the results from multiple samples, the probability of an incorrect result is reduced, and the accuracy is increased. The balance of quality and speed can be adjusted in a straightforward manner, simply by adjusting the number of samples taken.</p>
<p>In room acoustics, stochastic algorithms may be based directly on reflection paths, using <em>ray tracing</em> or <em>beam tracing</em>, in which rays or beams are considered to transport acoustic energy around the scene. Alternatively, they may use a surface-based technique, such as <em>acoustic radiance transfer</em> (ART), in which surfaces are used as intermediate stores of acoustic energy.</p>
<p>Surface-based methods, especially, are suited to real-time simulations (i.e.Â interactive, where the listener position can change), as the calculation occurs in several passes, only the last of which involves the receiver object. This means that early passes can be computed and cached, and only the final pass must be recomputed if the receiver position changes.</p>
<p>The main deterministic method is the <em>image source</em> method, which is designed to calculate the exact reflection paths between a source and a receiver. For shoebox-shaped rooms, and perfectly rigid surfaces, it is able to produce an exact solution to the wave equation. However, by its nature, it can only model specular (perfect) reflections, ignoring diffuse and diffracted components. For this reason, it is inexact for arbitrary enclosures, and unsuitable for calculating reverb tails, which are predominantly diffuse. The technique also becomes prohibitively expensive beyond low orders of reflection. The naive implementation reflects the sound source against all surfaces in the scene, resulting in a set of <em>image sources</em>. Then, each of these image sources is itself reflected against all surfaces. For high orders of reflection, the required number of calculations quickly becomes impractical. For these reasons, the image source method is only suitable for early reflections, and is generally combined with a stochastic method to find the late part of an impulse response (IR).</p>
<p>For a detailed reference on geometric acoustic methods, see <span class="citation" data-cites="savioja_overview_2015">[<a href="#ref-savioja_overview_2015">2</a>]</span>.</p>
<h3 id="wave-based-methods">Wave-based Methods</h3>
<p>The main advantage of wave-based methods is that they inherently account for wave effects such as diffraction and interference <span class="citation" data-cites="shelley_diffuse_2007">[<a href="#ref-shelley_diffuse_2007">5</a>]</span>, while geometric methods do not. This means that these wave-based methods are capable of accurately simulating the low-frequency component of a room IR, where constructive and destructive wave interference form <em>room modes</em>. Room modes have the effect of amplifying and attenuating specific frequencies in the room IR, and produce much of the subjective sonic âcolourâ or âcharacterâ of a room. Reproducing these room modes is therefore vital for evaluating the acoustics of rooms such as concert halls and recording studios, or when producing musically pleasing reverbs.</p>
<p>Wave-based methods may be derived from the <em>Finite Element Method</em> (FEM), <em>Boundary Element Method</em> (BEM) or <em>Finite-Difference Time-Domain</em> (FDTD) method. The FEM and BEM are known together as <em>Element Methods</em>.</p>
<p>The FEM is an iterative numerical method for finding natural resonances of a bounded enclosure. It models the air pressure inside the enclosure using a grid of interconnected nodes, each of which represents a mechanical system with a single degree of freedom. The interconnectedness of the nodes leads to a set of simultaneous equations, which can be solved for displacement at each node, and then the solved equations can be used to calculate pressure values at certain elements. The BEM is similar, but models nodes on the surface of the enclosure, instead of within it. This in turn allows it to model unbounded spaces, whereas the FEM is limited to bounded spaces <span class="citation" data-cites="murphy_digital_2000">[<a href="#ref-murphy_digital_2000">6</a>, pp. 52â55]</span>.</p>
<p>The FDTD method works by dividing the space to be modelled into a regular grid, and computing changes in some quantity (such as pressure or particle velocity) at each grid point over time. The formula used to update each grid point, along with the topology of the grid, may be varied depending on the accuracy, efficiency, and complexity required by the application. FDTD methods are generally applied to problems in electromagnetics, but a subclass of the FDTD method known as the <em>Digital Waveguide Mesh</em> (DWM) is often used for solving acoustics problems.</p>
<p>The FDTD process shares some characteristics with the element methods. They all become rapidly more computationally expensive as the maximum output frequency increases <span class="citation" data-cites="valimaki_fifty_2012">[<a href="#ref-valimaki_fifty_2012">7</a>]</span>. They also share the problem of discretisation or quantisation, in which details of the modelled room can only be resolved to the same accuracy as the spatial sampling period. If a large inter-element spacing is used, details of the room shape will be lost, whereas a small spacing will greatly increase the computational load.</p>
<p>The major advantage of FDTD over element methods is that it is run directly in the time domain, rather than producing frequency-domain results, which in turn affords a much simpler implementation.</p>
<p>FDTD simulations can also be implemented with relative efficiency by taking advantage of their âembarrassingly parallelâ nature. Each individual node in the simulation (of which there may be thousands or millions) can be updated without synchronisation. As a result, the nodes may be updated entirely in parallel, leading to massive reductions in simulation time.</p>
<p>The main disadvantage of the FDTD method is that it is susceptible to <em>numerical dispersion</em>, in which wave components travel at different speeds depending on their frequency and direction, especially at high frequencies. Several techniques exist to reduce this error, such as oversampling the mesh <span class="citation" data-cites="campos_computational_2005">[<a href="#ref-campos_computational_2005">8</a>]</span>, using different mesh topologies <span class="citation" data-cites="savioja_reduction_1999 van_duyne_tetrahedral_1995">[<a href="#ref-savioja_reduction_1999">9</a>], [<a href="#ref-van_duyne_tetrahedral_1995">10</a>]</span>, and post-processing the simulation output <span class="citation" data-cites="savioja_interpolated_2001">[<a href="#ref-savioja_interpolated_2001">11</a>]</span>. Oversampling further increases the computational load of the simulation, while using different topologies and post-processing both introduce additional complexity.</p>
<p>Despite its drawbacks, the FDTD method is generally preferred for room acoustics simulation <span class="citation" data-cites="valimaki_fifty_2012">[<a href="#ref-valimaki_fifty_2012">7</a>]</span>, due to its straightforward implementation, inherent parallelism, intuitive behaviour, and its ability to directly produce time-domain IRs.</p>
<h2 id="existing-software">Existing Software</h2>
<p>A handful of programs exist for acoustic simulation. Table <a href="#tbl:software">1</a> shows a selection which, whilst not exhaustive, is representative.</p>
<a name="tbl:software"></a>
<table>
<caption>Table 1: Some of the most prominent tools for acoustic simulation. </caption>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Availability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Odeon <span class="citation" data-cites="_odeon_2016">[<a href="#ref-_odeon_2016">12</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Commercial</td>
</tr>
<tr class="even">
<td style="text-align: left;">CATT-Acoustic <span class="citation" data-cites="_catt-acoustic_2016">[<a href="#ref-_catt-acoustic_2016">13</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Commercial</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Olive Tree Lab <span class="citation" data-cites="_otl_2016">[<a href="#ref-_otl_2016">14</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Commercial</td>
</tr>
<tr class="even">
<td style="text-align: left;">EASE <span class="citation" data-cites="_ease_2016">[<a href="#ref-_ease_2016">15</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Commercial</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Auratorium <span class="citation" data-cites="_audioborn_2016">[<a href="#ref-_audioborn_2016">16</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Commercial</td>
</tr>
<tr class="even">
<td style="text-align: left;">RAVEN <span class="citation" data-cites="schroder_raven:_2011">[<a href="#ref-schroder_raven:_2011">17</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RoomWeaver <span class="citation" data-cites="beeson_roomweaver:_2004">[<a href="#ref-beeson_roomweaver:_2004">18</a>]</span></td>
<td style="text-align: left;">Waveguide</td>
<td style="text-align: left;">None</td>
</tr>
<tr class="even">
<td style="text-align: left;">EAR <span class="citation" data-cites="_ear_2016">[<a href="#ref-_ear_2016">19</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Free</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PachydermAcoustic <span class="citation" data-cites="_pachyderm_2016">[<a href="#ref-_pachyderm_2016">20</a>]</span></td>
<td style="text-align: left;">Geometric</td>
<td style="text-align: left;">Free</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parallel FDTD <span class="citation" data-cites="_parallelfdtd_2016">[<a href="#ref-_parallelfdtd_2016">21</a>]</span></td>
<td style="text-align: left;">Waveguide</td>
<td style="text-align: left;">Free</td>
</tr>
<tr class="odd">
<td style="text-align: left;">i-Simpa <span class="citation" data-cites="_i-simpa_2016">[<a href="#ref-_i-simpa_2016">22</a>]</span></td>
<td style="text-align: left;">Geometric, extensible</td>
<td style="text-align: left;">Free</td>
</tr>
</tbody>
</table>
<p>All commercial acoustics programs found use geometric techniques, probably because they are fast to run, and can often be implemented to run interactively, in real-time. However, low-frequency performance is a known issue with these programs. For example, the FAQ page for the Odeon software <span class="citation" data-cites="_odeon_2016-1">[<a href="#ref-_odeon_2016-1">23</a>]</span> notes that:</p>
<blockquote>
<p>For Odeon simulations as with real measurements, the source and receiver should be at least 1/4th wave length from the walls. But at the very lowest resonance of the room the level can change a lot from position to position without Odeon being able to predict it. For investigation of low frequency behavior (resonances), indeed Odeon is not the tool.</p>
</blockquote>
<p>Clearly there is a need for wave-modelling acoustics software, which can accurately predict low frequency behaviour. However, such software seems to be somewhat rarer than geometric acoustics software. Of the two wave-modelling programs listed, only one is generally available, which must additionally be run from Python or Matlab scripts. This is a good approach for research software, but would probably not be straightforward for users with limited programming experience.</p>
<p>At time of writing (December 2016) it appears that no generally-available (commercially or otherwise) piece of software has taken the approach of combining wave-modelling and geometric methods, although this technique is well-known in the literature <span class="citation" data-cites="southern_hybrid_2013 aretz_combined_2009 murphy_hybrid_2008 southern_room_2013 vorlander_simulation_2009 southern_spatial_2011">[<a href="#ref-southern_spatial_2011">1</a>], [<a href="#ref-aretz_combined_2009">4</a>], [<a href="#ref-southern_hybrid_2013">24</a>]â[<a href="#ref-vorlander_simulation_2009">27</a>]</span>.</p>
<h2 id="acoustic-simulation-and-the-creative-arts">Acoustic Simulation and the Creative Arts</h2>
<p>Musicians and sound designers can choose from an abundance of convolution reverb plugins, such as Wavesâ <em>IR1 Convolution Reverb</em> <span class="citation" data-cites="waves_ir1">[<a href="#ref-waves_ir1">28</a>]</span>, Audio Easeâs <em>Altiverb</em> <span class="citation" data-cites="altiverb">[<a href="#ref-altiverb">29</a>]</span>, and Liquid Sonicsâ <em>Reverberate 2</em> <span class="citation" data-cites="reverberate2">[<a href="#ref-reverberate2">30</a>]</span>. These tools are extremely flexible: by convolving an IR with some other signal, that signal can be made to sound as though it was recorded in the same location as the IR. A music producer might use this technique to create a recording of an âorchestraâ in which each instrument is recorded separately, composited, and convolved with the IR of a concert hall. Similarly, a foley artist could use convolution reverb to make studio-recorded effects sound more believable in the context of the environment on-screen.</p>
<p>The main drawback of convolution reverbs is their dependency upon high-quality IR recordings. Although most tools come with a library of IRs, this library will not be comprehensive. In some circumstances (for example, when attempting to seamlessly combine foley effects with location recordings) a suitable pre-recorded IR will not be directly available. In other circumstances, it may not even be possible to record a suitable IR using traditional methods, because the desired reverb is designed to evoke an environment that does not (or cannot) exist.</p>
<p>In these situations, the user has a few options. Firstly, a custom IR could be recorded. This will require specialist equipment, and access to the particular location. Secondly, the desired reverb could be approximated using an algorithmic (i.e.Â not convolution-based) reverb tool. Thirdly, the IR could be predicted using an acoustic simulator. The third approach seems like a sensible middle ground between the first two options. It should be less time-consuming than recording a custom IR, and will not require access to the modelled location (although a 3D virtual model would be necessary). Additionally, the simulated IR should match real-world behaviour more closely than an algorithmic reverb.</p>
<p>Despite the obvious application of virtual acoustics to music and sound production, all of the software in table <a href="#tbl:software">1</a> appears to be targeted at technical users with specialist knowledge in acoustics. For example, the i-Simpa homepage <span class="citation" data-cites="_i-simpa_2016">[<a href="#ref-_i-simpa_2016">22</a>]</span> says:</p>
<blockquote>
<p>It is a perfect tool for experts (i.e.Â acousticians), for teachers and students, as well as for researchers, in their projects (room acoustics, urban acoustics, industrial spaces, acoustic coursesâ¦)</p>
</blockquote>
<p>The Olive Tree Lab âphilosophyâ page <span class="citation" data-cites="philosophy_otl_2016">[<a href="#ref-philosophy_otl_2016">31</a>]</span> describes a similar focus on technical users:</p>
<blockquote>
<p>â¦we hope to assist acousticians and engineers in predicting sound and noise propagation more accurately, especially in the field of Noise Control.</p>
</blockquote>
<p>In the case of the âEASEâ software, its name is an acronym standing for âEnhanced Acoustic Simulator for Engineersâ.</p>
<p>This targeting of technical users has an effect on the program design. The programs prioritise physical accuracy, and the ability to export visualisations and statistics about the modelled acoustics. These tools may also have steep learning curve, assuming that users are already familiar with acoustics theory. A simulation tool for creative users should have different goals:</p>
<ul>
<li><strong>Sound quality</strong>: Generated IRs should be suitable for use without any additional cleanup or editing.</li>
<li><strong>Intuitive controls</strong>: The interface should make it obvious how each parameter will affect the output.</li>
<li><strong>Simulation speed</strong>: Part of the creative process is experimentation, and users need to hear the effects of their experiments quickly in order to iterate towards the desired sound.</li>
</ul>
<p>Creative users only require a subset of the functionality provided by other simulators. That is, they only require the final IR result. Other features, such as the creation and export of statistics and visualisations, are not required. Therefore, such a tool could be reasonably streamlined, presenting a simple âimport, configure, renderâ workflow and omitting additional analysis features.</p>
<h2 id="project-aims">Project Aims</h2>
<p>Based on the evidence presented, it seems that there is a clear need for an acoustic simulation tool which uses wave-modelling to predict low-frequency behaviour, and which is targeted at creative users. The goal of the Wayverb project is build such a tool. The development of this program should prioritise the following goals:</p>
<ul>
<li><strong>Plausibility</strong>: Provide a way of generating physically plausible impulse responses of arbitrary enclosed spaces.</li>
<li><strong>Efficiency</strong>: Ensure that the simulation is fast. Simulations should take less than ten minutes in general, and certainly never more than an hour.</li>
<li><strong>Accessibility</strong>: The programâs controls should be intuitive, and it should be possible for someone with no programming or acoustics experience to generate IRs.</li>
</ul>
<p>The ideal simulation program would be capable of replicating, with perfect accuracy, the real-world behaviour of any acoustic scenario. However, for the purposes of sound-design, this level of accuracy is not necessary. When creating a reverb, a sound designerâs focus is generally on experimenting and developing the desired atmosphere, rather than on perfectly reconstructing a physical location. Therefore, simulation results should be believable first:</p>
<ul>
<li>The room size should directly affect the reverb duration, with smaller rooms exhibiting lower reverb times than larger spaces.</li>
<li>The absorptions of the surfaces in the room should also affect reverb times. Absorptive surfaces should produce lower reverb times than more reflective surfaces. This effect should also be frequency-dependent, so that a room which is largely reflective at low frequencies and absorptive at high frequencies will exhibit appropriate relative reverb times in each frequency band.</li>
<li>Rooms with parallel surfaces should show modal resonances at the correct frequencies.</li>
<li>Adjusting the separating distance between a source and receiver should lead to corresponding changes in the balance between early and late reflections. For larger separations, the direct contribution should become less pronounced.</li>
<li>If the source is not directly visible from the receiver, there should be no direct contribution.</li>
<li>Reflective tunnel-like rooms should produce distinct echoes in the reverb tail.</li>
<li>Modelled microphones should attenuate directional contributions appropriately, according to the polar pattern of the capsule.</li>
</ul>
<p>Another aspect of plausibility is overall quality: if a generated IR contains obvious artefacts, it is by definition physically implausible, and of limited use to a sound designer.</p>
<p>Plausibility and efficiency are competing goals, which must be balanced. Extreme performance, allowing real-time usage, has already been implemented in several of the commercial programs listed above, and generally relies on simplified acoustic models, which in turn reduce accuracy. This runs counter to the aims of the project. Similarly, high-quality simulations generally require long compute times and specialised hardware, both of which are inaccessible to the target user. Therefore, the focus of the project cannot be solely on plausibility. Software which balances these two aims does not exist, at time of writing, and there is a clear need for a solution which runs on commodity hardware, and is both reasonably fast and produces believable results. Ideally, this software would allow the user control over the trade-off between accuracy and efficiency, enabling fast, lower-quality simulations to be used when auditioning, and a slower, higher-quality render to be produced once the user is happy with all the simulation settings.</p>
<p>Regarding accessibility, the program must be simple to install and run, and users should not require specialist training in acoustics or programming in order to become productive. Controls must be intuitive, and it should be obvious how each parameter will affect the final IR. The project as a whole should be accessible too: the code and supporting materials must be made freely available to researchers, to encourage further research and modification. Note that accessibility is a lesser goal than plausibility and efficiency: if the program is fast and produces high quality, usable results, users will be prepared to invest time to learn the program. Meanwhile, if the program is easy to use but is too slow or produces poor results, users will have no reason to learn the software in the first place.</p>
<h3 id="proposed-solution">Proposed Solution</h3>
<p>It appears that an approach combining geometric and wave-based methods will be most flexible in achieving both plausibility and efficiency: wave-based methods are accurate but slow; and geometric methods are faster but more approximate. Efficiency can be balanced against output quality by adjusting the proportion of the output generated with each method. The Wayverb project puts forward an acoustic simulator based on this hybrid method.</p>
<p>To achieve the goal of accessibility, the Wayverb program runs on consumer hardware, and is accessed through a graphical interface which allows simulations to be configured, stored, and run. Code for the project is public and permissively licensed.</p>
<h2 id="original-contributions">Original Contributions</h2>
<p>Most importantly, at time of writing, Wayverb is the only public graphical acoustics tool incorporating geometric and wave-based methods. Although hybrid acoustics methods are well documented <span class="citation" data-cites="southern_hybrid_2013 aretz_combined_2009 murphy_hybrid_2008">[<a href="#ref-aretz_combined_2009">4</a>], [<a href="#ref-southern_hybrid_2013">24</a>], [<a href="#ref-murphy_hybrid_2008">25</a>]</span>, they have only been used in specific research settings, for producing experimental results. It may be assumed that these tools have been built to model specific test-cases, rather than general simulation tasks, but this is uncertain as no tools incorporating these techniques have been made public. However, Wayverb is able to model arbitrary enclosures.</p>
<p>The project acts as a survey of room acoustics techniques, and related issues regarding practical implementation. Rather than designing completely new simulation methods, existing techniques were investigated, compared, and evaluated in terms of their plausibility and performance. Then, optimum techniques were chosen and further developed for use in the final program. An especially important consideration is the matching of parameters between models. For example, all models should produce the same sound energy at a given distance, and should exhibit the same reverb time for a given scene. Therefore, the acoustics techniques were chosen so that they produce consistent results.</p>
<p>Sometimes the models required development beyond the methods presented in the literature in order to become useful. An example of this is the waveguide set-up process. Most experimental set-ups in the literature only model cuboid-shaped enclosures, and no guidance is given for setting up simulations in arbitrarily-shaped enclosures. Of course, it must be possible to model real, complex room shapes, and so an original set-up procedure had to be developed. The same goes for memory layout and implementation details: in the literature, techniques for efficient implementation are rarely discussed. As a result, new techniques had to be invented, rather than reimplementing known methods. Where extensions to existing techniques have been developed for use in Wayverb, this is mentioned in the text.</p>
<p>Much of the literature on acoustic simulation focuses predominantly on accuracy. Performance appraisals are rarely given, presumably because they are somewhat subjective, and âreasonableâ efficiency will vary between applications. Ideally, the simulation methods in Wayverb should be selected and implemented to allow tunable performance, so that results with acceptable accuracy can be generated within a few minutes, but it is possible to run longer simulations if higher-accuracy results are needed. This is similar to approaches taken in computer graphics, where âoverviewâ renders may take seconds to generate, but physically-modelled simulations for film often take hours to render, even on purpose-built compute clusters.</p>
<p>The notable components of the Wayverb project are as follows, each of which has a dedicated chapter with detailed explanation:</p>
<ul>
<li>Image-source model, accelerated with parallel ray-casting, for early reflections. Uses a novel method for speeding up audibility tests by re-using reflection paths from the ray tracer.</li>
<li>Parallel stochastic ray-tracer, for late reflections.</li>
<li>Parallel digital waveguide mesh, for low frequency modelling. Uses a novel set-up procedure to create meshes with correctly-placed boundary nodes in arbitrary scenes.</li>
<li>Calibration, automatically matching the output levels of the different models.</li>
<li>A microphone model, capable of simulating capsules with direction-dependent frequency responses, within all three simulation-types.</li>
<li>A boundary model with matched performance in all three simulation-types.</li>
</ul>
<h3 id="chosen-simulation-techniques">Chosen Simulation Techniques</h3>
<p>The image-source and stochastic ray-tracing methods were chosen for modelling high-frequency content. These models are complementary: the image model can find early reflections with great accuracy but is slow at finding later reflections; while the ray-tracer is much faster but more approximate, making it better suited to finding naturally-diffuse late reflections. Specifically, a simple ray tracing method was chosen over a phonon- or surface-based method for the late-reflection simulation, for several reasons. Firstly, ray tracing is broadly discussed in the literature <span class="citation" data-cites="krokstad_calculating_1968 kuttruff_room_2009 vorlander_auralization:_2007 schroder_physically_2011 alpkocak_computing_2010">[<a href="#ref-krokstad_calculating_1968">32</a>]â[<a href="#ref-alpkocak_computing_2010">36</a>]</span>, so would not require a great deal of experimentation to implement. Secondly, ray tracing has the property of being an <em>embarrassingly parallel</em> algorithm, because each individual ray can be simulated entirely independently, without requiring communication or synchronisation. By running the algorithm on graphics hardware, which is designed to run great numbers of calculations in parallel, all rays could be simulated in one go, yielding much greater performance than processing each ray sequentially. Finally, though surface-based methods are capable of real-time operation, they do not pose any performance benefit for non-real-time or âone-offâ simulations. Their performance comes from re-using pre-computed information when the receiver position changes, but in a one-off simulation the receiver position is fixed. Ray tracing is also less complex and better documented than surface-based methods, making it the superior choice for this application. A logistical reason for choosing the image-source and ray tracing solution for high-frequency modelling was that the author had previously implemented such a system for an undergraduate project. It was hoped that much of the code from that project could be re-used, but it transpired that the project suffered from accuracy and implementation issues, making it unsuitable for direct integration within Wayverb. Therefore, the majority of this code was completely re-written. The author was, however, able to re-use much of the knowledge and experience gained from the previous project, which would not have been possible if a completely new stochastic method had been introduced.</p>
<p>For low-frequency simulation, a FDTD-based DWM model was chosen. There is a great deal of writing on this method <span class="citation" data-cites="van_duyne_3d_1996 savioja_interpolated_2014 kowalczyk_room_2011 campos_computational_2005 murphy_digital_2000">[<a href="#ref-murphy_digital_2000">6</a>], [<a href="#ref-campos_computational_2005">8</a>], [<a href="#ref-van_duyne_3d_1996">37</a>]â[<a href="#ref-kowalczyk_room_2011">39</a>]</span>, it is relatively simple to implement, and shares with ray tracing the characteristic of being embarrassingly parallel. As wave-modelling is especially costly, a parallel implementation is necessary in order to achieve simulation times in the order of minutes rather than hours or days.</p>
<p>An in-depth description of the algorithms implemented is given in the <a href="/wayverb/image_source.html">Image-Source</a>, <a href="/wayverb/ray_tracer.html">Ray Tracer</a>, and <a href="/wayverb/waveguide.html">Waveguide</a> sections. Figure <a href="#fig:regions">2</a> shows how the outputs from the different methods work together to produce a broadband IR. It shows that the lower portion of the spectrum is generated entirely with the waveguide, while the upper portion is simulated using the image-source method for early reflections, and the ray tracing method for the reverb tail.</p>
<figure>
<img src="images/regions.svg" alt="Figure 2: The structure of a simulated IR." id="fig:regions" /><figcaption>Figure 2: The structure of a simulated IR.</figcaption>
</figure>
<p>Deciding on the simulation techniques led to three questions:</p>
<ul>
<li>To produce a final output, the three simulations must be automatically mixed in some way. How can this be done?</li>
<li>Binaural simulation requires some method for direction- and frequency-dependent attenuation at the receiver. How can receivers with polar patterns other than omnidirectional be modelled consistently in all three simulation methods?</li>
<li>The reverb time and character depends heavily on the nature of the reflective surfaces in the scene. How can frequency-dependent reflective boundaries be modelled consistently in all methods?</li>
</ul>
<p>These questions will be discussed in the <a href="/wayverb/hybrid.html">Hybrid</a>, <a href="/wayverb/microphone.html">Microphone Modelling</a>, and <a href="/wayverb/boundary.html">Boundary Modelling</a> sections respectively.</p>
<h3 id="chosen-technology">Chosen Technology</h3>
<p>The programming language chosen was C++. For acceptable performance in numerical computing, a low-level language is required, and for rapid prototyping, high-level abstractions are necessary. C++ delivers on both of these requirements, for the most part, although its fundamentally unsafe memory model does introduce a class of bugs which do not really exist in languages with garbage collection, borrow checking, or some other safety mechanism.</p>
<p>OpenCL was chosen for implementing the most parallel parts of the simulation. The OpenCL framework allows a single source file to be written, in a C-like language, which can target either standard <em>central processing units</em> (CPUs), or highly parallel <em>graphics processing units</em> (GPUs). The main alternative to OpenCL is CUDA, which additionally can compile C++ code, but which can only target Nvidia hardware. OpenCL was chosen as it would allow the final program to be run on a wider variety of systems, with fewer limitations on their graphics hardware.</p>
<p>The only deployment target was Mac OS. This was mainly to ease development, as maintaining software across multiple platforms is often time-consuming. Mac OS also tends to have support for newer C++ language features than Windows, which allows code to be more concise, flexible, and performant.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Targeting a single platform avoids the need to use only the lowest common denominator of language features. As far as possible, the languages and libraries have been selected to be portable if the decision to support other platforms is made in the future.</p>
<p>The following additional libraries were used to speed development. They are all open-source and freely available.</p>
<dl>
<dt>GLM</dt>
<dd>Provides vector and matrix primitives and operations, primarily designed for use in 3D graphics software, but useful for any program that will deal with 3D space.
</dd>
<dt>Assimp</dt>
<dd>Used for loading and saving 3D model files in a wide array of formats, with a consistent interface for querying loaded files. Using a 3D mesh importer means that users can load and simulate models created in practically any mesh editor, providing the mesh is manifold and represents a single watertight enclosure.
</dd>
<dt>FFTW3</dt>
<dd>Provides Fast Fourier Transform routines. Used mainly for filtering and convolution.
</dd>
<dt>Libsndfile</dt>
<dd>Used for loading and saving audio files, specifically for saving simulation results.
</dd>
<dt>Libsamplerate</dt>
<dd>Provides high-quality sample-rate-conversion routines. Waveguide simulations are often run at a relatively low sample-rate, which must then be adjusted.
</dd>
<dt>Gtest</dt>
<dd>A unit-testing framework, used to validate small individual parts of the program, and ensure that changes to one module do not cause breakage elsewhere.
</dd>
<dt>Cereal</dt>
<dd>Serializes data to and from files. Used for saving program configuration options.
</dd>
<dt>ITPP</dt>
<dd>A scientific computing library. Used for its implementation of the Yule-Walker method for estimating filter coefficients for a given magnitude response.
</dd>
<dt>JUCE</dt>
<dd>Provides a framework for building graphical applications in C++. Used for the final application.
</dd>
</dl>
<p>The project uses CMake to configure its build, and to automatically download project dependencies. Python and Octave were used for running and automating tests and generating graphs.</p>
<p>This documentation is written in Markdown, and compiled to HTML and to PDF using Pandoc. The project website is generated with Jekyll.</p>
<h2 id="summary">Summary</h2>
<p>An account of techniques commonly used for room acoustics simulation has been provided. The strengths and weaknesses of these techniques have been discussed, leading to the observation that geometric and wave-based models have complementary characteristics. The weaknesses of the individual models could be minimised by creating a combined âhybridâ model. This hybrid approach has not previously been used in publicly-available software, and such acoustic simulation software that is available is consistently targeted at technical users. This evidence suggests the need for a program focused on the requirements of creative users, which uses a hybrid modelling approach, and which is made publicly available. Specific goals for such a program have been suggested and explained, and the original contributions of the program have been examined. Finally, a plan to build the program has been put forward.</p>
<h1 id="bibliography" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-southern_spatial_2011">
<p>[1] A. Southern, S. Siltanen, and L. Savioja, âSpatial room impulse responses with a hybrid modeling method,â in <em>Audio Engineering Society Convention 130</em>, 2011. </p>
</div>
<div id="ref-savioja_overview_2015">
<p>[2] L. Savioja and U. P. Svensson, âOverview of geometrical room acoustic modeling techniques,â <em>The Journal of the Acoustical Society of America</em>, vol. 138, no. 2, pp. 708â730, 2015. </p>
</div>
<div id="ref-smith_physical_1992">
<p>[3] J. O. Smith, âPhysical modeling using digital waveguides,â <em>Computer music journal</em>, vol. 16, no. 4, pp. 74â91, 1992. </p>
</div>
<div id="ref-aretz_combined_2009">
<p>[4] M. Aretz, R. NÃ¶then, M. VorlÃ¤nder, and D. SchrÃ¶der, âCombined broadband impulse responses using FEM and hybrid ray-based methods,â in <em>EAA Symposium on Auralization</em>, 2009. </p>
</div>
<div id="ref-shelley_diffuse_2007">
<p>[5] S. B. Shelley, <em>Diffuse boundary modelling in the digital waveguide mesh</em>. University of York, 2007. </p>
</div>
<div id="ref-murphy_digital_2000">
<p>[6] D. T. Murphy and D. M. Howard, âDigital waveguide mesh topologies in room acoustics modelling,â PhD thesis, Citeseer, 2000. </p>
</div>
<div id="ref-valimaki_fifty_2012">
<p>[7] V. VÃ¤limÃ¤ki, J. D. Parker, L. Savioja, J. O. Smith, and J. S. Abel, âFifty years of artificial reverberation,â <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 20, no. 5, pp. 1421â1448, 2012. </p>
</div>
<div id="ref-campos_computational_2005">
<p>[8] G. R. Campos and D. M. Howard, âOn the computational efficiency of different waveguide mesh topologies for room acoustic simulation,â <em>IEEE Transactions on Speech and Audio Processing</em>, vol. 13, no. 5, pp. 1063â1072, 2005. </p>
</div>
<div id="ref-savioja_reduction_1999">
<p>[9] L. Savioja and V. VÃ¤limÃ¤ki, âReduction of the dispersion error in the interpolated digital waveguide mesh using frequency warping,â in <em>Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on</em>, 1999, vol. 2, pp. 973â976. </p>
</div>
<div id="ref-van_duyne_tetrahedral_1995">
<p>[10] S. A. Van Duyne and J. O. Smith, âThe tetrahedral digital waveguide mesh,â in <em>Applications of Signal Processing to Audio and Acoustics, 1995., IEEE ASSP Workshop on</em>, 1995, pp. 234â237. </p>
</div>
<div id="ref-savioja_interpolated_2001">
<p>[11] L. Savioja and V. VÃ¤limÃ¤ki, âInterpolated 3-D digital waveguide mesh with frequency warping,â in <em>Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSPâ01). 2001 IEEE International Conference on</em>, 2001, vol. 5, pp. 3345â3348. </p>
</div>
<div id="ref-_odeon_2016">
<p>[12] âOdeon.â 2016 [Online]. Available: <a href="http://www.odeon.dk/" class="uri">http://www.odeon.dk/</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_catt-acoustic_2016">
<p>[13] âCATT-Acoustic.â 2016 [Online]. Available: <a href="http://www.catt.se/" class="uri">http://www.catt.se/</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_otl_2016">
<p>[14] âOTL.â 2016 [Online]. Available: <a href="http://www.olivetreelab.com/Room" class="uri">http://www.olivetreelab.com/Room</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_ease_2016">
<p>[15] âEASE.â 2016 [Online]. Available: <a href="http://ease.afmg.eu/index.php/features.html" class="uri">http://ease.afmg.eu/index.php/features.html</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_audioborn_2016">
<p>[16] âAudioborn.â 2016 [Online]. Available: <a href="http://www.audioborn.com" class="uri">http://www.audioborn.com</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-schroder_raven:_2011">
<p>[17] D. SchrÃ¶der and M. VorlÃ¤nder, âRAVEN: A real-time framework for the auralization of interactive virtual environments,â in <em>Forum Acusticum</em>, 2011. </p>
</div>
<div id="ref-beeson_roomweaver:_2004">
<p>[18] M. J. Beeson and D. T. Murphy, âRoomWeaver: A digital waveguide mesh based room acoustics research tool,â in <em>Proc. COST G6 Conf. Digital Audio Effects (Naples, Italy, October 2004)</em>, 2004, pp. 268â73. </p>
</div>
<div id="ref-_ear_2016">
<p>[19] âEar,â <em>GitHub</em>. 2016 [Online]. Available: <a href="https://github.com/aothms/ear" class="uri">https://github.com/aothms/ear</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_pachyderm_2016">
<p>[20] âPachyderm Acoustic,â <em>GitHub</em>. 2016 [Online]. Available: <a href="https://github.com/PachydermAcoustic" class="uri">https://github.com/PachydermAcoustic</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_parallelfdtd_2016">
<p>[21] âParallelFDTD,â <em>GitHub</em>. 2016 [Online]. Available: <a href="https://github.com/juuli/ParallelFDTD" class="uri">https://github.com/juuli/ParallelFDTD</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_i-simpa_2016">
<p>[22] âI-Simpa,â <em>I-Simpa</em>. 2016 [Online]. Available: <a href="http://i-simpa.ifsttar.fr/" class="uri">http://i-simpa.ifsttar.fr/</a>. [Accessed: 05-Dec-2016]</p>
</div>
<div id="ref-_odeon_2016-1">
<p>[23] âOdeon FAQ.â 2016 [Online]. Available: <a href="http://www.odeon.dk/faq-page#t16n151" class="uri">http://www.odeon.dk/faq-page#t16n151</a>. [Accessed: 08-Dec-2016]</p>
</div>
<div id="ref-southern_hybrid_2013">
<p>[24] A. Southern and S. Siltanen, âA hybrid acoustic model for room impulse response synthesis,â in <em>Proceedings of Meetings on Acoustics</em>, 2013, vol. 19, p. 015113. </p>
</div>
<div id="ref-murphy_hybrid_2008">
<p>[25] D. Murphy, M. Beeson, S. Shelley, A. Moore, and A. Southern, âHybrid room impulse response synthesis in digital waveguide mesh based room acoustics simulation,â in <em>Proceedings of the 11th International Conference on Digital Audio Effects (DAFx-08)</em>, 2008, pp. 129â136. </p>
</div>
<div id="ref-southern_room_2013">
<p>[26] A. Southern, S. Siltanen, D. T. Murphy, and L. Savioja, âRoom impulse response synthesis and validation using a hybrid acoustic model,â <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 21, no. 9, pp. 1940â1952, 2013. </p>
</div>
<div id="ref-vorlander_simulation_2009">
<p>[27] M. VorlÃ¤nder, âSimulation and auralization of broadband room impulse responses,â in <em>TecniacÃºstica 2009</em>, 2009. </p>
</div>
<div id="ref-waves_ir1">
<p>[28] âWaves iR1 convolution reverb.â 2017 [Online]. Available: <a href="http://www.waves.com/plugins/ir1-convolution-reverb" class="uri">http://www.waves.com/plugins/ir1-convolution-reverb</a>. [Accessed: 22-Jul-2017]</p>
</div>
<div id="ref-altiverb">
<p>[29] âAltiverb.â 2017 [Online]. Available: <a href="https://www.audioease.com/altiverb/" class="uri">https://www.audioease.com/altiverb/</a>. [Accessed: 22-Jul-2017]</p>
</div>
<div id="ref-reverberate2">
<p>[30] âReverberate 2.â 2017 [Online]. Available: <a href="https://www.liquidsonics.com/software/reverberate-2/" class="uri">https://www.liquidsonics.com/software/reverberate-2/</a>. [Accessed: 22-Jul-2017]</p>
</div>
<div id="ref-philosophy_otl_2016">
<p>[31] âOur philosophy.â 2017 [Online]. Available: <a href="http://www.mediterraneanacoustics.com/philosophy.html" class="uri">http://www.mediterraneanacoustics.com/philosophy.html</a>. [Accessed: 05-Jul-2017]</p>
</div>
<div id="ref-krokstad_calculating_1968">
<p>[32] A. Krokstad, S. Strom, and S. SÃ¸rsdal, âCalculating the acoustical room response by the use of a ray tracing technique,â <em>Journal of Sound and Vibration</em>, vol. 8, no. 1, pp. 118â125, 1968. </p>
</div>
<div id="ref-kuttruff_room_2009">
<p>[33] H. Kuttruff, <em>Room Acoustics, Fifth Edition</em>. CRC Press, 2009. </p>
</div>
<div id="ref-vorlander_auralization:_2007">
<p>[34] M. VorlÃ¤nder, <em>Auralization: Fundamentals of acoustics, modelling, simulation, algorithms and acoustic virtual reality</em>. Springer Science &amp; Business Media, 2007. </p>
</div>
<div id="ref-schroder_physically_2011">
<p>[35] D. SchrÃ¶der, <em>Physically based real-time auralization of interactive virtual environments</em>, vol. 11. Logos Verlag Berlin GmbH, 2011. </p>
</div>
<div id="ref-alpkocak_computing_2010">
<p>[36] A. Alpkocak and M. Sis, âComputing impulse response of room acoustics using the ray-tracing method in time domain,â <em>Archives of Acoustics</em>, vol. 35, no. 4, pp. 505â519, 2010. </p>
</div>
<div id="ref-van_duyne_3d_1996">
<p>[37] S. A. Van Duyne and J. O. Smith III, âThe 3D tetrahedral digital waveguide mesh with musical applications,â in <em>Proceedings of the 1996 International Computer Music Conference</em>, 1996, pp. 9â16. </p>
</div>
<div id="ref-savioja_interpolated_2014">
<p>[38] L. Savioja, T. Lokki, and V. VÃ¤limÃ¤ki, âThe interpolated 3-D digital waveguide mesh method for room acoustic simulation and auralization,â <em>Ultragarsasâ Ultrasoundâ</em>, vol. 48, no. 3, pp. 48â52, 2014. </p>
</div>
<div id="ref-kowalczyk_room_2011">
<p>[39] K. Kowalczyk and M. van Walstijn, âRoom acoustics simulation using 3-D compact explicit FDTD schemes,â <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 19, no. 1, pp. 34â46, 2011. </p>
</div>
<div id="ref-_visual_2016">
<p>[40] âVisual Studio support for C++ language features.â 2016 [Online]. Available: <a href="https://msdn.microsoft.com/en-us/library/hh567368.aspx" class="uri">https://msdn.microsoft.com/en-us/library/hh567368.aspx</a>. [Accessed: 09-Dec-2016]</p>
</div>
<div id="ref-_clang_2016">
<p>[41] âClang support for C++ language features.â 2016 [Online]. Available: <a href="http://clang.llvm.org/cxx_status.html" class="uri">http://clang.llvm.org/cxx_status.html</a>. [Accessed: 09-Dec-2016]</p>
</div>
<div id="ref-_download_2016">
<p>[42] âDownload LLVM releases.â 2016 [Online]. Available: <a href="http://llvm.org/releases/" class="uri">http://llvm.org/releases/</a>. [Accessed: 09-Dec-2016]</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Visual Studio 2015 for Windows still does not support all of the C++11 language features <span class="citation" data-cites="_visual_2016">[<a href="#ref-_visual_2016">40</a>]</span>, while the Clang compiler used by Mac OS has supported newer C++14 features since version 3.4 <span class="citation" data-cites="_clang_2016">[<a href="#ref-_clang_2016">41</a>]</span>, released in May 2014 <span class="citation" data-cites="_download_2016">[<a href="#ref-_download_2016">42</a>]</span>.<a href="#fnref1">â©</a></p></li>
</ol>
</section>

        <nav id="prev_next_nav">
    
    
        
    
        
            
            
            
                <a href="/wayverb/introduction.html" class="prev_page">Introduction</a>
            

            
            
            
                <a href="/wayverb/theory.html" class="next_page">Theory</a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
</nav>

    </div>
    <footer id="footer" class="wrapper alt">
    <div class="inner">
        <ul class="menu">
			<li>
                &copy; Reuben Thomas 2016. All rights reserved.
            </li>
            <li>
                Design: <a href="http://html5up.net">HTML5 UP</a>, modified by Reuben Thomas.
            </li>
		</ul>
	</div>
</footer>

<!-- Scripts -->
<script src="/wayverb/assets/js/jquery.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrollex.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrolly.min.js"></script>
<script src="/wayverb/assets/js/skel.min.js"></script>
<script src="/wayverb/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/wayverb/assets/js/main.js"></script>

</div>
</body>
</html>
