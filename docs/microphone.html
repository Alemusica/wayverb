<!DOCTYPE HTML>
<html>
<head>
    <title>Wayverb - Microphone modelling</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" type="image/x-icon" href="/wayverb/assets/favicon.ico" />
	<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/wayverb/assets/css/main.css" />
    <link rel="stylesheet" href="/wayverb/assets/css/font-awesome.min.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/wayverb/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/wayverb/assets/css/ie8.css" /><![endif]-->

<!-- Scripts -->
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>

<body>
<nav id="sidebar_nav">
    <a href="/wayverb/" class="title">Wayverb</a>
    <ul>
        
        
            <li>
                <a href="/wayverb/introduction.html" >
                    Introduction
                </a>
            </li>
        
            <li>
                <a href="/wayverb/context.html" >
                    Context
                </a>
            </li>
        
            <li>
                <a href="/wayverb/image_source.html" >
                    Image-source
                </a>
            </li>
        
            <li>
                <a href="/wayverb/ray_tracer.html" >
                    Ray tracer
                </a>
            </li>
        
            <li>
                <a href="/wayverb/waveguide.html" >
                    Waveguide
                </a>
            </li>
        
            <li>
                <a href="/wayverb/hybrid.html" >
                    Hybrid
                </a>
            </li>
        
            <li>
                <a href="/wayverb/microphone.html" class="active">
                    Microphone modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/boundary.html" >
                    Boundary modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/evaluation.html" >
                    Evaluation and Future
                </a>
            </li>
        
            <li>
                <a href="/wayverb/demos.html" >
                    Demos
                </a>
            </li>
        
            <li>
                <a href="/wayverb/bibliography.html" >
                    References
                </a>
            </li>
        
    </ul>
</nav>

<div id="page_main">
    <header>
        <ul>
            <li class="nav_menu open" >
                <a href="#sidebar_nav">
                    &#9776;
                </a>
            </li>
            <li class="nav_menu close" >
                <a href="#">
                    &#9776;
                </a>
            </li>
            <li>
                <a href="/wayverb/" >
                    Wayverb
                </a>
            </li>
        </ul>
    </header>
    <div class="inner">
        <nav id="prev_next_nav">
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
            
            
                <a href="/wayverb/hybrid.html" class="prev_page">Hybrid</a>
            

            
            
            
                <a href="/wayverb/boundary.html" class="next_page">Boundary modelling</a>
            
        
    
        
    
        
    
        
    
        
    
</nav>

        <div id="TOC">
<ul>
<li><a href="#microphone-modelling">Microphone Modelling</a><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#background">Background</a></li>
<li><a href="#image-source-implementation">Image Source Implementation</a></li>
<li><a href="#ray-tracer-implementation">Ray Tracer Implementation</a></li>
<li><a href="#dwm-implementation">DWM Implementation</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</div>
<h1 id="microphone-modelling" class="major">Microphone Modelling</h1>
<h2 id="introduction">Introduction</h2>
<p>In the preceding sections, simulation results have been recorded using a virtual omnidirectional microphone model. This model records the sound pressure at a single point in space. The gain and frequency response of the receiver are not affected by the direction of the pressure gradient. Signals recorded with omnidirectional microphones are primarily useful for assessing the overall “character” of an acoustic space, including reverb time, intelligibility, and overall gain.</p>
<p>However, one of the goals of room acoustics simulation is <em>virtual acoustics</em>: recreating the modelled auditory environment in such a way that the listener believes they are hearing sounds within a physical space. Real reverbs are highly directional, containing direct sound from the direction of the source, and reflected sounds from the room boundaries. To produce a convincing virtual environment, these directional cues must be recorded and reproduced. Omnidirectional microphones cannot record directional responses, and so a different direction-sensitive receiver model must be developed and used instead.</p>
<h2 id="background">Background</h2>
<p>Humans are able to detect the originating direction of sounds using two main techniques: Interaural Time Difference (ITD) defines the time delay between ears, when the sound source is located nearer to one ear than the other; and Interaural Level Difference (ILD) denotes the frequency-dependent difference in level caused by attenuation by the outer ear, head, and torso. The success of virtual acoustics depends on the ability to produce signals with appropriate ITD and ILD, in such a way that the sound is perceived as coming from a certain direction.</p>
<p>When recording impulse responses of physical spaces, several techniques might be used to retain ITD and ILD information.</p>
<p>One option is to record the impulse response using a matched pair of microphones. A spaced pair of omnidirectional microphones (known as an AB pair) will capture interchannel time difference, but will not capture interchannel level distance if the source is positioned far away from the microphones. Alternatively, a coincident pair of cardioid capsules (an XY pair) or bidirectional capsules (a Blumlein) pair might be used. These configurations capture level difference, but are incapable of recording time difference, because sound will always arrive at both capsules at the same time.</p>
<p>Microphone pair methods are only suitable for recording stereo signals, as they only capture a two-dimensional “slice” through the modelled scene, where all directional information is restricted to the same plane as the microphone capsules. The technique can be extended to higher dimensions by using more microphone capsules. This is the basis of the ambisonic technique, which uses four coincident directional microphone capsules to capture the three-dimensional directional pressure gradient. Instead of being used directly, the recorded signals are post-processed depending on the configuration of the output speakers. A three dimensional speaker-array can perfectly reproduce the sound field captured by the microphones. For playback on headphones, the signals can be filtered with <em>head related transfer functions</em> (HRTFs), which modify the frequency content of the sound depending on its originating direction, mimicking the absorptive characteristics of the human head and torso <span class="citation" data-cites="noisternig_3d_2003">[<a href="#ref-noisternig_3d_2003">1</a>]</span>. As the microphone capsules are all coincident, ITD cannot be perfectly reconstructed (the ITD differs depending on the direction and distance of the source, and ambisonic techniques cannot record distance, only the pressure gradient direction). Therefore, ambisonics is not the optimal choice for headphone playback.</p>
<p>The preferred method for capturing impulse responses which retain ILD <em>and</em> ITD in a format suitable for headphone playback is the “dummy head” microphone pair. This is a model of a human torso, constructed from materials which mimic the acoustic properties of a real torso, with a microphone in each ear. Recordings made with a dummy head capture the ITD and ILD that would be perceived by a real human listener, and produce a convincing reconstruction of the sonic space when played back over headphones. However, responses recorded using this technique are unsuitable for loudspeaker playback.</p>
<p>All of these techniques require that the receiver gain is dependent upon the incident direction. The dummy head technique additionally requires that the frequency response of the receiver is dependent upon the incident direction. To allow reproduction on both headphones and arbitrary loudspeaker arrays, the receiver model should encompass all of the techniques described above. Given that spaced microphone techniques can be modelled simply by including multiple receivers in the simulation, each virtual capsule should have configurable direction-dependent gain and frequency response, allowing it to behave like a standard microphone, or like the ear of a dummy head.</p>
<h2 id="image-source-implementation">Image Source Implementation</h2>
<p>As described in the <a href="/wayverb/image_source.html">Image Source</a> section, the magnitude of the contribution of each image source depends on the distance between the source and receiver, and the direction-dependent reflectance characteristics of any intermediate reflective surfaces. To model a directional receiver, the contribution magnitude is additionally scaled by the receiver’s gain in the direction of that contribution.</p>
<p>It is very easy to obtain the direction of each image source: simply subtract the receiver position from the image source position, and normalise the result. This direction can either be used to directly compute the appropriate attenuation factor, or it can be used to look up the attenuation in a pre-computed table.</p>
<p>It is also easy to adapt this technique, in the case that the frequency response of the receiver depends on incident direction, rather than just the overall gain. In Wayverb, image-source contributions are calculated in eight frequency bands, which are filtered and mixed down at the end of the simulation. A direction-dependent frequency response can be modelled simply by using a different attenuation function or look-up table for each frequency band. Using this method, interchannel level difference can be modelled with accuracy matching the rest of the image-source simulation.</p>
<!-- TODO mention interchannel time difference method for image source? -->
<h2 id="ray-tracer-implementation">Ray Tracer Implementation</h2>
<p>In the ray tracer, a very similar approach can be taken. The incident direction of each ray is known, so this direction can be used with a function or lookup-table to adjust the ray’s multiband intensity before it is added to the energy histogram (see <a href="/wayverb/ray_tracer.html">Ray Tracer</a>).</p>
<p>In terms of implementation, this approach is very expensive. In Wayverb, the initial simulation and directional processing stages are kept separate, allowing a single simulation to be processed by several different virtual microphone capsules. While flexible, it is also infeasible to store the direction of each incident ray, especially if there are hundreds of thousands of rays in the simulation. The ray tracer is used to estimate the late reverberation of the space, in which individual sonic events are “blurred” together, and in which there are few directional cues <span class="citation" data-cites="schroder_physically_2011">[<a href="#ref-schroder_physically_2011">2</a>, p. 21]</span>, so this level of per-ray directional accuracy is unnecessary.</p>
<p>Instead, the surface of the spherical receiver is divided into discrete <em>directivity groups</em> <span class="citation" data-cites="schroder_physically_2011">[<a href="#ref-schroder_physically_2011">2</a>, p. 72]</span>. A separate energy histogram is maintained for each directivity group, where the direction of ray incidence determines the histogram used to store that ray’s energy.</p>
<!-- TODO raytracer process diagram -->
<!-- TODO tests? -->
<h2 id="dwm-implementation">DWM Implementation</h2>
<p>TODO</p>
<ul>
<li><p>what were the implementation options and decisions</p></li>
<li><p>why did I choose the final implementation</p></li>
<li><p>how does it work</p></li>
<li><p>tests - <em>does</em> it work?</p></li>
</ul>
<h1 id="bibliography" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-noisternig_3d_2003">
<p>[1] M. Noisternig, T. Musil, A. Sontacchi, and R. Holdrich, “3d binaural sound reproduction using a virtual ambisonic approach,” in <em>Virtual Environments, Human-Computer Interfaces and Measurement Systems, 2003. VECIMS’03. 2003 IEEE International Symposium on</em>, 2003, pp. 174–178 [Online]. Available: <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1227050" class="uri">http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1227050</a>. [Accessed: 03-Jan-2017]</p>
</div>
<div id="ref-schroder_physically_2011">
<p>[2] D. Schröder, <em>Physically based real-time auralization of interactive virtual environments</em>, vol. 11. Logos Verlag Berlin GmbH, 2011 [Online]. Available: <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=HtDxSt0jcRkC&amp;oi=fnd&amp;pg=PR17&amp;dq=+Dirk+Schro+%CC%88der+Physically+Based+Real-Time+Auralization+of+Interactive+Virtual+Environments&amp;ots=CCbCVBNlmn&amp;sig=CAlxZysU-fxjnSkG_X1_dffQ72o">https://books.google.com/books?hl=en&amp;lr=&amp;id=HtDxSt0jcRkC&amp;oi=fnd&amp;pg=PR17&amp;dq=+Dirk+Schro+%CC%88der+Physically+Based+Real-Time+Auralization+of+Interactive+Virtual+Environments&amp;ots=CCbCVBNlmn&amp;sig=CAlxZysU-fxjnSkG_X1_dffQ72o</a>. [Accessed: 05-Dec-2016]</p>
</div>
</div>

        <nav id="prev_next_nav">
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
            
            
                <a href="/wayverb/hybrid.html" class="prev_page">Hybrid</a>
            

            
            
            
                <a href="/wayverb/boundary.html" class="next_page">Boundary modelling</a>
            
        
    
        
    
        
    
        
    
        
    
</nav>

    </div>
    <footer id="footer" class="wrapper alt">
    <div class="inner">
        <ul class="menu">
			<li>
                &copy; Reuben Thomas 2016. All rights reserved.
            </li>
            <li>
                Design: <a href="http://html5up.net">HTML5 UP</a>, modified by Reuben Thomas.
            </li>
		</ul>
	</div>
</footer>

<!-- Scripts -->
<script src="/wayverb/assets/js/jquery.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrollex.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrolly.min.js"></script>
<script src="/wayverb/assets/js/skel.min.js"></script>
<script src="/wayverb/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/wayverb/assets/js/main.js"></script>

</div>
</body>
</html>
