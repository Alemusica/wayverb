<!DOCTYPE HTML>
<html>
<head>
    <title>Wayverb - Evaluation</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" type="image/x-icon" href="/wayverb/assets/favicon.ico" />
	<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/wayverb/assets/css/main.css" />
    <link rel="stylesheet" href="/wayverb/assets/css/font-awesome.min.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/wayverb/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/wayverb/assets/css/ie8.css" /><![endif]-->

<!-- Scripts -->
<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>

<body>
<nav id="sidebar_nav">
    <a href="/wayverb/" class="title">Wayverb</a>
    <ul>
        
        
            <li>
                <a href="/wayverb/introduction.html" >
                    Introduction
                </a>
            </li>
        
            <li>
                <a href="/wayverb/context.html" >
                    Context
                </a>
            </li>
        
            <li>
                <a href="/wayverb/image_source.html" >
                    Image-source Model
                </a>
            </li>
        
            <li>
                <a href="/wayverb/ray_tracer.html" >
                    Ray tracer
                </a>
            </li>
        
            <li>
                <a href="/wayverb/waveguide.html" >
                    Waveguide
                </a>
            </li>
        
            <li>
                <a href="/wayverb/hybrid.html" >
                    Hybrid Model
                </a>
            </li>
        
            <li>
                <a href="/wayverb/microphone.html" >
                    Microphone modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/boundary.html" >
                    Boundary modelling
                </a>
            </li>
        
            <li>
                <a href="/wayverb/evaluation.html" class="active">
                    Evaluation
                </a>
            </li>
        
            <li>
                <a href="/wayverb/demos.html" >
                    Demos
                </a>
            </li>
        
            <li>
                <a href="/wayverb/bibliography.html" >
                    References
                </a>
            </li>
        
    </ul>
</nav>

<div id="page_main">
    <header>
        <ul>
            <li class="nav_menu open" >
                <a href="#sidebar_nav">
                    &#9776;
                </a>
            </li>
            <li class="nav_menu close" >
                <a href="#">
                    &#9776;
                </a>
            </li>
            <li>
                <a href="/wayverb/" >
                    Wayverb
                </a>
            </li>
        </ul>
    </header>
    <div class="inner">
        <nav id="prev_next_nav">
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
            
            
                <a href="/wayverb/boundary.html" class="prev_page">Boundary modelling</a>
            

            
            
            
                <a href="/wayverb/demos.html" class="next_page">Demos</a>
            
        
    
        
    
        
    
</nav>

        <div id="TOC">
<ul>
<li><a href="#evaluation">Evaluation</a><ul>
<li><a href="#features">Features</a></li>
<li><a href="#tests">Tests</a><ul>
<li><a href="#terms-and-measurements">Terms and Measurements</a></li>
<li><a href="#reverb-times-for-varying-room-volumes">Reverb Times for Varying Room Volumes</a></li>
<li><a href="#reverb-times-for-varying-absorptions">Reverb Times for Varying Absorptions</a></li>
<li><a href="#direct-response-time">Direct Response Time</a></li>
<li><a href="#obstructions">Obstructions</a></li>
<li><a href="#late-reflection-details">Late Reflection Details</a></li>
<li><a href="#directional-contributions">Directional Contributions</a></li>
<li><a href="#binaural-modelling">Binaural Modelling</a></li>
</ul></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#simulation-method">Simulation Method</a></li>
<li><a href="#general-implementation">General Implementation</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</div>
<h1 id="evaluation" class="major">Evaluation</h1>
<p>This section describes the Wayverb program, and demonstrates some example simulation results. The simulations are chosen to highlight the behaviour of the simulator with respect to parameters such as reverb time, frequency content, and early reflection times. The project files for each of these tests is included in the Wayverb distribution.</p>
<h2 id="features">Features</h2>
<p>The Wayverb program has the following features:</p>
<ul>
<li><strong>Hybrid Geometric and Waveguide Simulation</strong>: This is the most important feature of Wayverb, providing the ability to simulate the acoustics of arbitrary enclosed spaces.</li>
<li><strong>Load Arbitrary Models</strong>: The model-importing functionality is built on top of the Assimp library, which has support for a wide variety of 3D formats <span class="citation" data-cites="_assimp_2017">[<a href="#ref-_assimp_2017">1</a>]</span>. Care must be taken to export models with the correct scale, as Wayverb interprets model units as metres. The detected model dimensions are shown in the interface, so that model dimensions can be checked, and the model can be re-exported if necessary.</li>
<li><strong>Visualiser</strong>: Allows the state of the simulation to be observed, as it changes.</li>
<li><strong>Unlimited Sources and Receivers</strong>: Set up any number of sources and receivers. This has the trade-off that the simulation will automatically run once for each source-receiver pair, which will be time consuming when there are many combinations.</li>
<li><strong>Multiple Capsules per Receiver</strong>: Each receiver behaves like a set of coincident capsules. Each capsule may model an ideal microphone, or an HRTF ear. Multiple capsules at the same receiver require only a single simulation run, so multi-capsule receivers should be preferred over multiple receivers, wherever possible. For HRTF simulations, the receiver position will be automatically adjusted during the image-source simulation, replicating the interaural spacing of a real pair of ears (see the Image Source Implementation subsection of the <a href="/wayverb/microphone.html">Microphone Modelling</a> section). This produces a realistic stereo time-delay effect in the early-reflection portion of the output, aiding localisation.</li>
<li><strong>Custom Materials</strong>: Wayverb reads unique material names from the loaded 3D model. Each unique material in the model may be assigned custom acoustic properties, consisting of multi-band absorption and scattering coefficients.</li>
<li><strong>Tunable Ray Tracer</strong>: The number of rays is controlled by a quality parameter, which defines the number of rays which are expected to intersect the receiver per histogram interval. Higher quality values will lead to more accurate reverb tails, at the cost of longer processing times. The desired image-source depth can also be varied from 0 to 10, although lower values are recommended. In the real world, the ratio of scattered to non-scattered sound energy will increase as the impulse response progresses. The image-source model does not account for scattering. Therefore, lower image-source reflection depths are more physically plausible, as the simulation will switch to stochastic ray-tracing (which <em>does</em> account for scattering) sooner.</li>
<li><strong>Tunable Waveguide</strong>: The waveguide has two modes: a single-band mode which uses the Yule-Walker method to estimate boundary filter parameters, and a multi-band mode which uses “flat” filters. These filters are able to model a given wall absorption with greater accuracy, but only when the wall absorption is constant across the spectrum. The multi-band mode is therefore significantly slower, as it must run the waveguide process several times. It uses the wall absorption from each frequency band in turn, and then band-pass filters and mixes the results of each simulation to find the final output. Both waveguide modes allow the maximum waveguide frequency, and the oversampling factor, to be modified.</li>
</ul>
<p>The interface of the Wayverb program is explained in the following figure.</p>
<figure>
<img src="images/wayverb_ui.svg" alt="The interface of the Wayverb program." /><figcaption>The interface of the Wayverb program.</figcaption>
</figure>
<h2 id="tests">Tests</h2>
<p>Some aspects of the Wayverb algorithm have already been tested in previous sections, and so do not require further testing here.</p>
<p>Specifically, the <a href="/wayverb/hybrid.html">Hybrid Model</a> section compares the waveguide to an ideal image-source model, showing that the output level is correctly matched between models. This test also shows that the modal response of the waveguide matches the “ideal” response for several different values of absorption coefficients, implying that the waveguide and image-source boundary models are consistent.</p>
<p>The <a href="/wayverb/microphone.html">Microphone Modelling</a> section shows that the waveguide model is capable of simulating directionally-dependent receivers, with gain dependent on the angle of the incident wave-front.</p>
<p>Finally, the <a href="/wayverb/boundary.html">Boundary Modelling</a> section shows that the waveguide boundaries exhibit the expected wall impedance (though with some error, which increases at higher frequencies).</p>
<p>In the tests below, all impulse responses are produced using the Wayverb software. Reverb times are calculated using the Room EQ Wizard <span class="citation" data-cites="_room_2017">[<a href="#ref-_room_2017">2</a>]</span>. The test projects can be found in the Wayverb repository.</p>
<div id="audio_table">
<p>The following file is used as a carrier signal for testing the generated impulse responses. It starts with a single Dirac impulse, which will produce a single copy of the generated impulse response in the output. This is followed by a drum loop, a piano phrase, a guitar melody, and a short string sample, all taken from the Logic Pro loop library. The final sample is an operatic voice, taken from the OpenAir anechoic sound database <span class="citation" data-cites="_operatic_2017">[<a href="#ref-_operatic_2017">3</a>]</span> and reproduced under the terms of the Creative Commons BY-SA license. These sounds are chosen as they have no reverb applied, and so will give an accurate representation of the applied reverbs. Additionally, they cover a wide frequency range, and contain both sustained and transient material, presenting the reverbs under a range of conditions.</p>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">name</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">carrier signal</td>
<td style="text-align: left;"><audio controls><source src="demos/out_cardioid_away.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="terms-and-measurements">Terms and Measurements</h3>
<p>Here, reverb times are described using the <em>RT60</em>, which is a parameter denoting the time taken for the sound level in a space to fall by sixty decibels <span class="citation" data-cites="_reverberation_2017">[<a href="#ref-_reverberation_2017">4</a>]</span>. Commonly, impulse response recordings do not have the necessary dynamic range of 75dB or more which is necessary to directly calculate the RT60. Instead, a smaller level decrease is timed, and this time is extrapolated to give an estimate of the RT60. The time taken for a twenty or thirty decibel decrease is used, and multiplied by two or three respectively to estimate the RT60. These measurements are known as the T20 and T30.</p>
<p>The estimated or predicted RT60, <span class="math inline">\(T\)</span>, of a given space can be calculated using the Sabine formula <span class="citation" data-cites="kuttruff_room_2009">[<a href="#ref-kuttruff_room_2009">5</a>, p. 131]</span>:</p>
<ol class="example" type="1">
<li><span class="math display">\[T=0.161\frac{V}{A}\]</span></li>
</ol>
<p>where <span class="math inline">\(V\)</span> is the volume of the space in cubic metres, and <span class="math inline">\(A\)</span> is the <em>equivalent absorption area</em>. The absorption area of a given surface is equal to the area of the surface multiplied by its absorption coefficient. The equivalent absorption area for an entire room is found by summing the absorption areas of all surfaces in the scene.</p>
<h3 id="reverb-times-for-varying-room-volumes">Reverb Times for Varying Room Volumes</h3>
<p>This test aims to check that rooms with different volumes produce the expected reverb times. Rooms with different volumes, but the same absorption coefficients and source/receiver positions, are simulated. Then, the RT60 is calculated from the simulated impulse responses, and compared against the Sabine estimate. A close match shows that the change in room volume has the correct, physically plausible effect on the generated outputs.</p>
<p>Three different cuboid rooms with the following dimensions are modelled:</p>
<ul>
<li><strong>small</strong>: <span class="math inline">\(2 \times 2.5 \times 3\)</span> metres</li>
<li><strong>medium</strong>: <span class="math inline">\(4.5 \times 2.5 \times 3.5\)</span> metres</li>
<li><strong>large</strong>: <span class="math inline">\(12 \times 4 \times 8\)</span> metres</li>
</ul>
<p>Each room is set to have absorption and scattering coefficients of 0.1 in all bands. The source and receiver are placed 1 metre apart at the centre of each room. The waveguide is used to model frequencies up to 500Hz, using a mesh with a sampling rate of 3330Hz. The image-source model generates reflections up to the fourth order.</p>
<p>The following results are found, for the entire (broadband) output:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 20%" />
<col style="width: 25%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">room</th>
<th style="text-align: left;">Sabine RT / s</th>
<th style="text-align: left;">measured T20 / s</th>
<th style="text-align: left;">measured T30 / s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">small</td>
<td style="text-align: left;">0.653</td>
<td style="text-align: left;">0.663</td>
<td style="text-align: left;">0.658</td>
</tr>
<tr class="even">
<td style="text-align: left;">medium</td>
<td style="text-align: left;">0.887</td>
<td style="text-align: left;">0.897</td>
<td style="text-align: left;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: left;">large</td>
<td style="text-align: left;">1.76</td>
<td style="text-align: left;">1.86</td>
<td style="text-align: left;">1.96</td>
</tr>
</tbody>
</table>
<p>The results for small and medium rooms are within 5% of the expected reverb time, although the measured T30 of the larger room has an error of 11%. Increasing the room volume has the effect of increasing the reverb time, as expected.</p>
<p>Now, the results are plotted in octave bands. The results in lower bands, which are modelled by the waveguide, have a significantly shorter reverb time than the upper bands, which are generated geometrically. The higher bands have reverb times slightly higher than the Sabine prediction, while the waveguide-generated bands show much shorter reverb times tails than expected. The difference in reverb times between the waveguide and geometric methods also becomes evident when spectrograms are taken of the impulse responses. In all tests, the initial level is constant across the spectrum, but dies away faster at lower frequencies. In the medium and large rooms, some resonance at 400Hz is seen towards the end of the reverb tail, which is probably caused by numerical dispersion in the waveguide mesh.</p>
<p>In the medium and large tests, the spectrograms appear as though the low-frequency portion has a longer, rather than a shorter, reverb time. However, in the large test, the late low-frequency energy has a maximum of around -100dB, which is 40dB below the level of the initial contribution. The measured T20 and T30 values do not take this into account, and instead reflect the fact that the <em>initial</em> reverb decay is faster at low frequencies. The spectrograms show that the waveguide sometimes resonates for an extended period at low amplitudes. In effect, the waveguide exhibits a high noise-floor under some conditions.</p>
<figure>
<img src="images/room_size_rt30.svg" alt="T30 in octave bands, calculated from the measured impulse responses." /><figcaption>T30 in octave bands, calculated from the measured impulse responses.</figcaption>
</figure>
<figure>
<img src="images/room_size_spectrograms.svg" alt="Spectrograms of impulse responses obtained from different room sizes." /><figcaption>Spectrograms of impulse responses obtained from different room sizes.</figcaption>
</figure>
<p>This result is difficult to explain. A shorter reverb time indicates that energy is removed from the model at a greater rate than expected. Energy in the waveguide model is lost only at boundaries, so the most likely explanation is that these boundaries are too absorbent. It is also possible that the microphone model causes additional unexpected attenuation.</p>
<p>Further tests (not shown) of the three rooms were carried out to check possible causes of error. In one test, the Yule-Walker-generated boundary filters were placed with filters representing a constant real-valued impedance across the spectrum, to check whether the boundary filters had been generated incorrectly. In a second test, the modelled omnidirectional microphone at the receiver was removed, and the raw pressure value at the output node was used instead, to check that the microphone was not introducing undesired additional attenuation. However, in both tests, similar results were produced, with reverb times significantly lower than the Sabine prediction. The boundary and microphone models do not appear to be the cause of the problem.</p>
<p>The reverb-time test at the end of the <a href="/wayverb/hybrid.html">Hybrid Model</a> section shows that the waveguide reverb times match the reverb times of the exact image-source model, which will be close to the analytical solution in a cuboid room. The close match to the almost-exact image-source model suggests that the waveguide and boundary model have been implemented correctly. Additionally, the tests in the <a href="/wayverb/boundary.html">Boundary Modelling</a> section show that wall impedances are accurately modelled.</p>
<p>Given that in all previous tests the waveguide behaves as expected, one possibility is that the Sabine equation is simply a poor predictor of low-frequency reverb times, or reverb times in regularly-shaped rooms with well-defined reflection patterns (such as cuboids). If this were the case, this might justify the waveguide results. This is a reasonable suggestion: the Sabine equation assumes that the sound field is diffuse, which in turn requires that at any position within the room, reverberant sound has equal intensity in all directions, and random phase relations <span class="citation" data-cites="hodgson_when_1994">[<a href="#ref-hodgson_when_1994">6</a>]</span>. This is obviously untrue in a cuboid at low frequencies, where the non-random phase of reflected waves causes strong modal behaviour due to waves resonating between the parallel walls of the enclosure.</p>
<p>Further testing is required to locate the exact cause of the differences in reverb times. In the present implementation, the mismatch is an obvious artefact in the output, which affects the impulse response’s suitability for musical applications.</p>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">small</td>
<td style="text-align: left;"><audio controls><source src="demos/out_small.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="even">
<td style="text-align: left;">medium</td>
<td style="text-align: left;"><audio controls><source src="demos/out_medium.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="odd">
<td style="text-align: left;">large</td>
<td style="text-align: left;"><audio controls><source src="demos/out_large.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="reverb-times-for-varying-absorptions">Reverb Times for Varying Absorptions</h3>
<p>This test simulates the same room with several different absorption coefficients. The “medium” room from the above test is simulated, again with the source and receiver placed 1 metre apart in the centre of the room. Scattering is set to 0.1 in all bands. The absorption coefficients are set to 0.02, 0.04, and 0.08, corresponding to Sabine predictions of 4.43, 2.22, and 1.11 seconds.</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 20%" />
<col style="width: 25%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">absorption</th>
<th style="text-align: left;">Sabine RT / s</th>
<th style="text-align: left;">measured T20 / s</th>
<th style="text-align: left;">measured T30 / s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">4.433</td>
<td style="text-align: left;">4.295</td>
<td style="text-align: left;">4.283</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.04</td>
<td style="text-align: left;">2.217</td>
<td style="text-align: left;">2.210</td>
<td style="text-align: left;">2.219</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.08</td>
<td style="text-align: left;">1.108</td>
<td style="text-align: left;">1.126</td>
<td style="text-align: left;">1.156</td>
</tr>
</tbody>
</table>
<p>The issue with shorter low-frequency decay times persists in this test. However, the broadband reverb time responds correctly to the change in absorption coefficients.</p>
<figure>
<img src="images/room_material_spectrograms.svg" alt="Spectrograms of impulse responses obtained from simulating the same room with different absorption coefficients. Note that the low-frequency content has a shorter decay time than the high-frequency content." /><figcaption>Spectrograms of impulse responses obtained from simulating the same room with different absorption coefficients. Note that the low-frequency content has a shorter decay time than the high-frequency content.</figcaption>
</figure>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">absorption: 0.02</td>
<td style="text-align: left;"><audio controls><source src="demos/out_0.02.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="even">
<td style="text-align: left;">absorption: 0.04</td>
<td style="text-align: left;"><audio controls><source src="demos/out_0.04.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="odd">
<td style="text-align: left;">absorption: 0.08</td>
<td style="text-align: left;"><audio controls><source src="demos/out_0.08.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="direct-response-time">Direct Response Time</h3>
<p>The “large” room above is simulated again, but with the source and receiver positioned in diagonally opposite corners, both 1 metre away from the closest walls in all directions. The generated impulse response is compared to the previous impulse response, in which the source and receiver are placed 1 metre apart in the centre of the room.</p>
<p>Sound is simulated to travel at 340m/s, so when the source and receiver are placed 1m apart, a strong impulse is expected after 1/340 = 0.00294 seconds. In the new simulation, the source and receiver are placed <span class="math inline">\(\sqrt{10^2 + 2^2 + 6^2}\)</span> = 11.8m apart, corresponding to a direct contribution after 0.0348 seconds.</p>
<p>When the source is further away, the direct contribution may not be the loudest part of the impulse. As the distance from the source <span class="math inline">\(r\)</span> increases, the energy density of the direct component decreases proportionally to <span class="math inline">\(1/r^2\)</span>. However, the energy density in an ideally-diffuse room is independent of <span class="math inline">\(r\)</span>. At a certain distance, known as the <em>critical distance</em> <span class="math inline">\(r_c\)</span>, the energy densities of the direct component and reverberant field will match. Beyond the critical distance, the energy of the direct component will continue to decrease relative to the reverberant field <span class="citation" data-cites="kuttruff_room_2009">[<a href="#ref-kuttruff_room_2009">5</a>, pp. 146–147]</span>.</p>
<p>This effect is observed for the larger separating distance. The first and second order early reflections arrive very shortly after the direct response. Many of these reflection paths cover the same distance, and so arrive at the same time. These contributions are added, giving an instantaneous energy which is often greater than that of the direct contribution. The early reflections also occur at a greater frequency for the increased spacing. Therefore it is obvious that the ratio of energy densities between the direct and reverberant contributions is lower for greater spacings, as expected.</p>
<figure>
<img src="images/spacing_signals.svg" alt="Larger distances between the source and receiver result in delayed initial contributions. This figure shows the first 0.05s of the outputs of two simulations in which the source and receiver are separated by 1m and 11.8m." /><figcaption>Larger distances between the source and receiver result in delayed initial contributions. This figure shows the first 0.05s of the outputs of two simulations in which the source and receiver are separated by 1m and 11.8m.</figcaption>
</figure>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">near</td>
<td style="text-align: left;"><audio controls><source src="demos/out_large.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="even">
<td style="text-align: left;">far</td>
<td style="text-align: left;"><audio controls><source src="demos/out_large_spaced.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="obstructions">Obstructions</h3>
<p>Early reflection behaviour seems to be correct in simple cuboid models, where there is always line-of-sight between the source and receiver. The behaviour in more complex models, in which the source and receiver are not directly visible, must be checked.</p>
<p>The simulated space is a simple vault-like model, similar to a small hall, but broken up by regularly repeating pillars. The source and receiver are positioned seven metres apart, with their view obstructed by two pillars. If there were no obstruction, a strong direct impulse would be expected after 0.0206 seconds. However, the pillars should block this direct contribution.</p>
<figure>
<img src="images/vault_demo.png" alt="The testing set-up, showing the pillars blocking the line-of-sight between the source and receiver." /><figcaption>The testing set-up, showing the pillars blocking the line-of-sight between the source and receiver.</figcaption>
</figure>
<p>In the real world, objects with areas of a similar or greater order to the incident wavelength cause diffraction effects <span class="citation" data-cites="kuttruff_room_2009">[<a href="#ref-kuttruff_room_2009">5</a>, p. 59]</span>. The result of diffraction is that an additional “diffraction wave” is created at the edge of the object, which radiates in all directions. In the vault model, the edges of the pillars should cause diffraction, and in this way, some energy should be scattered from the source to the receiver. This energy will arrive slightly after the direct contribution would have, but before the first early reflection. The shortest possible path from source to receiver which travels around the pillars has a length of 7.12m, corresponding to a time of 0.0209s. Though the image-source and ray tracing models are not capable of modelling diffraction effects, the waveguide model inherently models this phenomenon. Therefore, the impulse response should record a low-frequency excitation at around 0.0209s.</p>
<figure>
<img src="images/vault_response.svg" alt="The early part of the vault impulse response. Low frequency diffraction from the waveguide is detected before the first image-source contribution." /><figcaption>The early part of the vault impulse response. Low frequency diffraction from the waveguide is detected before the first image-source contribution.</figcaption>
</figure>
<p>The impulse response graph  shows that low frequency diffraction is in fact recorded. Though this behaviour is physically correct, it highlights the main shortcoming of the hybrid algorithm. The correct behaviour of the waveguide conflicts with the approximate nature of the geometric algorithms, causing an obvious divide or disconnect between the low and high frequency regions in the output. The low frequencies have a fast onset and immediate decay, whereas the higher frequencies have a delayed onset with extended decay. This result is physically implausible, and makes the impulse response unsuitable for realistic, high-quality reverb effects.</p>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">vault</td>
<td style="text-align: left;"><audio controls><source src="demos/out_vault.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="late-reflection-details">Late Reflection Details</h3>
<p>Having checked the behaviour of early reflections, now the late-reflection performance must be checked. The nature of the ray tracing process means that fine detail (below 1ms precision) is not captured. However, it should be possible to observe reverb features on a larger scale, such as distinct echoes from a long tunnel.</p>
<p>A cuboid with dimensions <span class="math inline">\(4 \times 7 \times 100\)</span> metres is simulated. The receiver is placed exactly at the centre of the model, with the source positioned two metres away along the z direction. The output should contain a direct contribution at 0.00588s, and some early reflections from the nearby walls. The reverb tail should contain strong echoes every 0.294s, as the initial wave-front reflects back-and-forth between the two end walls.</p>
<p>The tunnel is modelled using absorption coefficients of 0.03 in the bottom five bands, then 0.04, and 0.07 in the highest two bands. The scattering coefficients are set to 0.1 in all bands. This scattering should cause echoes in the reverb tail to be “smeared” in time. To check the effect of the scattering coefficients, the same test is also run using scattering coefficients of 0 in all bands.</p>
<figure>
<img src="images/tunnel_spectrograms.svg" alt="Spectrograms of the tunnel impulse responses with different scattering levels. Note that the first 3 echoes are clear in both responses, but the scattered response quickly becomes less distinct, while the response with no scattering has clear echoes which die away more slowly." /><figcaption>Spectrograms of the tunnel impulse responses with different scattering levels. Note that the first 3 echoes are clear in both responses, but the scattered response quickly becomes less distinct, while the response with no scattering has clear echoes which die away more slowly.</figcaption>
</figure>
<p>The spectrograms show that there are clear increases in recorded energy, occurring approximately every 0.3 seconds after the initial onset. Each echo is followed by its own decay tail. In the case of the non-scattering simulation, the echoes are clearly separated, with very short tails. When the scattering is increased, the tails become longer, and the individual echoes become less defined. This is expected: when there is no scattering, rays travelling in any direction other than along the length of the tunnel will bounce between the walls many times, attenuated each time. These contributions may arrive at any time at the receiver, however, their amplitude will be greatly reduced. The rays travelling directly between the ends of the tunnel will be reflected fewer times, will lose less energy, and will produce louder regular contributions. When scattering is introduced, the ray paths are less regular, giving less correlation between the number of reflections and the time at which the ray is recorded. This in turn leads to a more even distribution of recorded energy over time.</p>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">tunnel with scattering</td>
<td style="text-align: left;"><audio controls><source src="demos/out_tunnel.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
<tr class="even">
<td style="text-align: left;">tunnel without scattering</td>
<td style="text-align: left;"><audio controls><source src="demos/out_tunnel_no_scatter.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="directional-contributions">Directional Contributions</h3>
<p>To test that microphone modelling behaves as expected, a cardioid microphone is placed in the exact centre of the “large” cuboid room, facing away from the source, which is positioned at a distance of 3m along the z-axis.</p>
<p>An ideal cardioid microphone has unity gain in its forward direction, and completely rejects waves incident from behind. In this test, the direct contribution from the source originates directly behind the receiver, so it should not be present in the output. The first contribution detected by the receiver is caused by reflections from the floor and ceiling. Both of these reflection paths have lengths of exactly 5m, corresponding to 0.0147s. These contributions will be incident from behind the microphone (but not directly behind) so they should have a relatively reduced magnitude.</p>
<p>The first contributions that strike the receiver from the front have path lengths of 11m and 13m, which should correspond to strong impulses at 0.0324s and 0.0382s.</p>
<p>These expectations are reflected in the results, shown in the following figure. The impulse response is silent at the time of the direct contribution, 0.00882s. The first significant level is recorded at 0.0147s, with the loudest contributions seen at 0.0324s and 0.0382s. These results are consistent across the spectrum, indicating that the microphone model is matched between both simulation methods.</p>
<figure>
<img src="images/cardioid.svg" alt="The response measured at a cardioid microphone pointing away from the source. The times marked, from left to right, are the direct contribution time, the first reflection time, and the first and second forward-incident reflection times." /><figcaption>The response measured at a cardioid microphone pointing away from the source. The times marked, from left to right, are the direct contribution time, the first reflection time, and the first and second <em>forward-incident</em> reflection times.</figcaption>
</figure>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">cardioid mic facing away</td>
<td style="text-align: left;"><audio controls><source src="demos/out_cardioid_away.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h3 id="binaural-modelling">Binaural Modelling</h3>
<p>Finally, the binaural model is tested. A concert hall is simulated, with a source placed in the centre of the stage area. A receiver is placed 10m away along both the x- and z-axes. The receiver is oriented so that it is facing directly down the z axis, meaning that the source is 14.1m away, on the left of the receiver.</p>
<p>The simulation produces output impulse responses for both the left and right ears. All of Wayverb’s simulation methods (image-source, ray-tracing and waveguide) use HRTF data to produce stereo effects based on interaural level difference. The image-source method additionally offsets the receiver position to produce interaural time difference effects, so in the outputs, slight time differences in the early reflections between channels are expected, and small level differences should be seen throughout both files.</p>
<p>In particular, the direct contribution would normally arrive at 14.1/340=0.0416s. However, the left “ear” is actually slightly closer to the source, and the right ear is slightly further away, which means that the first contribution should be at 0.0414s in the left channel, and 0.0418s in the right. The right ear is obstructed by the listener’s head, and should therefore have a reduced level relative to the left ear.</p>
<figure>
<img src="images/binaural_signals.svg" alt="Comparison of left- and right-ear responses, when the source is placed to the left of the receiver. Note the amplitude and time differences between the early reflections. The first contribution, in particular, is quieter in the right ear because it is occluded by the listener’s virtual head." /><figcaption>Comparison of left- and right-ear responses, when the source is placed to the left of the receiver. Note the amplitude and time differences between the early reflections. The first contribution, in particular, is quieter in the right ear because it is occluded by the listener’s virtual head.</figcaption>
</figure>
<p>The expected behaviour is observed in the outputs. The earliest contribution in the left channel occurs at 0.0414s, and has a greater level than the right channel’s contribution at 0.148s. The left-channel early reflections have an overall higher level than the early reflections in the right channel. However, as the impulse response progresses and becomes more diffuse, the energy levels equalise between channels.</p>
<div id="audio_table">
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">test</th>
<th style="text-align: left;">audio file</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">binaural receiver, source left</td>
<td style="text-align: left;"><audio controls><source src="demos/out_binaural.mp3" type="audio/mpeg">This browser does not support html5 audio.</audio></td>
</tr>
</tbody>
</table>
</div>
<h2 id="analysis">Analysis</h2>
<h3 id="simulation-method">Simulation Method</h3>
<p>All models perform as expected with regards to changes in room size and shape, material coefficients, source and receiver positions, and receiver microphone type. Reverb features such as distinct late echoes can be generated, and precise stereo effects, relying on both interaural time- and level-difference, can be created.</p>
<p>The main drawback, evident in several of the tests, is that the geometric and wave-based models have distinct behaviours. In the room-size and material tests, there are markedly different reverb times between the ray-tracer and waveguide outputs; and in the obstruction test, the waveguide exhibits diffraction behaviour which is not mirrored in the geometric output. These differences lead to obvious discontinuities in the frequency response, which persist despite calibrating the models to produce the same sound level at the same distance, and implementing matching boundary models in all models.</p>
<p>Some differences are to be expected. The primary reason for implementing multiple simulation methods is the relative accuracy of the different methods. Geometric algorithms are known to be inaccurate at low frequencies, a shortcoming that the waveguide was designed to overcome.</p>
<p>However, an accurate low-frequency response is useless if there is an obvious disconnect between high- and low-frequency outputs. For music and sound design applications, responses will sound <em>more</em> artificial if there is a rapid change in the frequency response, even if the low-frequency response taken alone is an accurate representation of the modelled space. Here, it is preferable that the frequency response not contain obvious discontinuities, even if this necessitates a reduction in overall accuracy.</p>
<p>Practical solutions to this problem are unclear. Ideally, the entire simulation would be run with the waveguide method, but this is impractical for all but the smallest simulations. Another option is to reduce the audible impact of the crossover between the waveguide and geometric outputs, simply by increasing its width. This has the dual drawbacks of decreasing the low-frequency accuracy, while also requiring a higher waveguide sample-rate, which is computationally expensive. Alternatively, the geometric algorithms may be altered, to account for effects such as diffraction, with the goal of minimising the differences between the wave and ray-based methods. This solution would maintain (or even improve) the accuracy of the simulation, but would again increase its cost. Finally, the goal of low-frequency accuracy could be abandoned, and the entire spectrum modelled using geometric methods. However, this would prevent important characteristics such as modal behaviour from being recorded.</p>
<h3 id="general-implementation">General Implementation</h3>
<p>The implementation has several known issues, other than those shown in the tests above. These issues can broadly be separated into two categories: firstly, problems with the simulation algorithm which were not highlighted in the above tests; and secondly, usability issues with the program interface.</p>
<h4 id="algorithm">Algorithm</h4>
<p>As noted in the <a href="/wayverb/waveguide.html">Digital Waveguide Mesh</a> section, the input signal used to excite the waveguide is not optimal. Its frequency response extends up to the Nyquist frequency, which means that the mesh has energy above the desired output frequency. As shown in the <a href="/wayverb/boundary.html">Boundary Modelling</a> section, the performance of the boundary filters is often erratic above 0.15 of the mesh sampling rate, sometimes increasing rather than reducing gain of incident signals. In combination, the broadband input signal sometimes causes the reflection filters to repeatedly amplify high-frequency content in the mesh. This is not audible in the final results, as the high frequency content is filtered out. However, it still leads to loss of precision, and sometimes numeric overflow. This might be solved by high-pass filtering the input signal, and then deconvolving the mesh output. However, it is not clear how such a process would interact with the microphone simulation. For example, it would probably be necessary to record and deconvolve the signals at all nodes surrounding the output node in order to produce a correct intensity vector. This would require further development and testing, for which there was insufficient time during the Wayverb project.</p>
<p>A similar drawback is to do with the low-frequency response of the mesh. Most input signals cause an increasing DC offset when the mesh uses a “soft” input node. To solve this, Wayverb’s mesh is excited using a “hard” node, which introduces low-frequency oscillations and reflection artefacts. An important area for future research is the development of an input signal which can be used in conjunction with a soft source, which does not cause any DC offset.</p>
<p>An important feature which is not implemented in Wayverb is the modelling of directional sources. Currently, all modelled sources emit a single spherical wave-front, which has equal energy in all directions. Real-world sources such as musical instruments and the human voice are directional. The ability to model directional sources would allow musicians and sound designers to create much more realistic and immersive acoustic simulations.</p>
<p>As well as directional sources, it might be useful to make the implementation of directional receivers more generic. Specifically, an ambisonic receiver would be useful, so that simulation results could be exported for directional processing in dedicated software. This could be achieved without modifying the geometric microphone model, in which coincident capsules are well-justified and lead to performance improvements (the simulation is run once for the entire capsule group, instead of once per individual capsule). However, the approach is not strictly justified in combination with the current waveguide microphone model <span class="citation" data-cites="hacihabiboglu_simulation_2010">[<a href="#ref-hacihabiboglu_simulation_2010">7</a>]</span>. Ambisonic output would therefore require further research into waveguide microphone modelling, in order to find a model which better supports coincident capsules.</p>
<p>The speed of the algorithm is lacking. The app was developed and tested on a recent laptop with a 2.5GHz processor, 16GB of RAM, and an MD Radeon R9 M370X graphics card with 2GB of dedicated VRAM. On this machine, several of the tests above took five or even ten minutes to run, which is a little too slow to be useful. When producing music, it is important to be able to audition and tweak reverbs in order to produce the most appropriate effect. With long rendering times, this auditioning process becomes protracted, which impacts the productivity of the musician. In addition, if the program uses the majority of the machine’s resources while it is running, then this prevents the user from running it in the background and continuing with other tasks in the meantime.</p>
<p>Alternatively, the speed may be acceptable for architectural applications, where absolute accuracy is most important. In this scenario, the focus is upon checking the acoustics of a previously-specified enclosure, rather than experimenting to find the most pleasing output. This means that longer times between renders can be justified if the results are highly accurate. Additionally, users may have access to more powerful server or specialised compute hardware which could lead to speed-ups.</p>
<p>The simulation methods have been implemented to minimise average-case computational complexity wherever possible, but both the ray-tracing and waveguide processes are limited to a complexity of <span class="math inline">\(O(n)\)</span> where n refers to the number of rays or nodes required in the simulation. Any further performance improvements may only be gained by improving the per-ray or per-node processing speed. This is certainly possible, but would yield relatively small improvements to the simulation speed. It may be equally valid to simply wait for hardware with increased parallelism support: a machine with twice as many graphics cores will run the program twice as fast. Such machines are likely to be commonplace in two or three years. Therefore, a better use of time would be to spend those two years focusing on the algorithm’s functional problems rather than optimisation.</p>
<h4 id="user-interface">User Interface</h4>
<p>The user interface is less fully-featured than may be expected of a professional simulation program. The reason for this is simple: the entire application was developed by a single developer, over sixteen months. To ensure that the application would reach a usable state, its scope had to be limited. In it’s first release, the application is only capable of loading, saving, configuring, and running simulations.</p>
<p>Originally, the app was designed to include built-in convolution support, so that generated impulse responses could be previewed without leaving the application. This feature would greatly improve the usability of the program. However, it would not contribute to the main goal of the program, which is the accurate and fast acoustic simulation of virtual environments. Convolution reverb tools already exist, and many users will have their own favourite programs and plug-ins for this purpose. The time that would have been spent replicating this functionality was better spent working on the unique and novel features of the program.</p>
<p>Similarly, the ability to edit the virtual spaces themselves from within the app was not implemented. Writing an intuitive editor for 3D objects would be a large undertaking, even for a team of developers. Instead, the ability to load a variety of 3D file formats was included, and users are advised to use dedicated software such as Blender or Sketchup to create their simulated spaces.</p>
<p>Some further usability features which are missing, which would ideally be included in a future release, include:</p>
<ul>
<li><strong>Undo and redo</strong>: If the user accidentally moves a source or receiver, or makes some other unwanted change, they must revert the change manually. There is no way of automatically reverting to the previous program state.</li>
<li><strong>Load and save of capsule and material presets</strong>: If the model contains several surfaces with different materials, and the user wants to apply a specific set of coefficients to all surfaces, each material must be configured by hand. There is no way to copy coefficients between surfaces, or to save and load materials from disk. Similarly, there is no way to save and load complex receiver set-ups from disk.</li>
<li><strong>Improved visualisation</strong>: Currently, ray energies are not communicated via the visualisation. There is also no way of adjusting the gain of the waveguide visualisation, which means that often the waveguide energy dies away quickly, becoming invisible, and giving the false impression that the simulation is not progressing.</li>
<li><strong>Command-line interface</strong>: For scripting or batch-processing of simulations, it would be useful to be able to run simulations from the command-line. Currently, this is only made possible by writing custom wrapper programs for the Wayverb library. It would be more useful to integrate command-line options directly into the Wayverb program.</li>
</ul>
<p>Finally, it was not possible to test the program extensively for crashes and bugs. The program was tested on a 15-inch MacBook Pro running OS 10.11, and a little on earlier models of 15- and 13-inch Macbook Pros running OS 10.10. On 10.11, and on the 13-inch laptop running 10.10, no crashes were evident, although on the 15-inch 10.10 machine there were a few crashes within OpenCL framework code. These crashes were reported by a user, from their personal machine. There was not sufficient time to fix these bugs during the project.</p>
<p>Extended access to this machine was not possible, and debugging OpenCL code without access to the problematic hardware is difficult. Depending on the drivers supplied by the GPU vendor, the kernel may be compiled in subtly different ways. For most (non-OpenCL) software, there will be a single binary image for a given processor architecture, and if the program crashes then a stack trace can be used to find the location of the bug. However, for OpenCL code, the executed binary is generated at runtime, and will vary depending on the specification and driver of the GPU. Also, crashes inside the OpenCL kernel do not emit a stack trace. Therefore, it is almost impossible to debug OpenCL code without access to the specific machine configuration which causes the issue.</p>
<p>A future release could fix these problems, but only with access to any problematic hardware and software configurations. As the program is open-source it would also be possible for third-parties experiencing bugs to contribute fixes.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The goal of the Wayverb project was to create a program which was capable of simulating the acoustics arbitrary enclosed spaces. For the program to be useful to its target audience of musicians and sound designers, it must be simultaneously accurate, efficient, and accessible.</p>
<p>The aims of accuracy and efficiency would be met by combining wave-modelling and geometric simulation methods, benefiting from both the physical realism of wave-modelling, and the computational performance of geometric simulation. This technique is not used by any other publicly available simulation package, so it was thought that a program implementing both models would be both faster and more accurate than competing programs. To further improve performance, the simulation would be implemented to run in parallel on graphics hardware. The program would be free and open-source, with a graphical interface, to ensure accessibility and encourage adoption.</p>
<p>Testing shows that the individual modelling methods are individually reasonably accurate. The ray-tracing and image-source methods respond appropriately to changes in room size, material, source/receiver spacing and receiver type. This is also true of the waveguide, which additionally is capable of modelling low-frequency modal responses, taking wave-effects such as diffraction into account. However, he accuracy of the waveguide is a drawback in some respects. When waveguide outputs are combined with geometric outputs, the relative inaccuracies of the geometric results are often highlighted by obvious discontinuities in the blended spectrum. Although use of the waveguide increases accuracy at low frequencies, generated outputs may be less useful than if generated entirely with geometric techniques, simply because of these discontinuities. This indicates that the goals of the project were misguided: an additional, primary goal of “usefulness” or “fitness” should have been considered. Future work may seek to improve the match between the outputs of the different models, perhaps sacrificing some low-frequency accuracy in the interests of sound quality.</p>
<p>In terms of efficiency, simulations generally complete within minutes, rather than hours or days, meeting the project’s efficiency target. It is also possible to observe the progression of the simulation, and to retry it with less intensive parameters, if it is progressing too slowly. Unfortunately, the time taken to generate outputs is not necessarily reflected in the quality of the results. For example, it is disappointing to wait for ten minutes for an impulse response, only to find that the output has markedly different reverb times at the top and bottom of the spectrum. Good user experience relies on users being able to generate results with acceptable quality as quickly as possible. If the user has to tweak and re-render, waiting for several minutes each time, before eventually finding appropriate settings, this translates to a poor user experience. This may be solved in two ways: by improving the quality of the outputs; and/or by further optimisation of the simulation algorithms.</p>
<p>The application has an accessible graphical interface. Although some desirable features (such as built-in convolution and 3D editing) are missing, the interface is focused and functional. It is possible to install and use without specialist training. Additionally, all code is open-source, allowing collaboration and contribution from interested third-parties. While the accuracy and efficiency goals were not conclusively met, it is clear that the finished project is sufficiently accessible.</p>
<p>Most importantly, the Wayverb project demonstrates that the hybrid modelling approach is viable in consumer software.</p>
<h1 id="bibliography" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-_assimp_2017">
<p>[1] “Assimp Supported Formats.” 2017 [Online]. Available: <a href="http://www.assimp.org/main_features_formats.html" class="uri">http://www.assimp.org/main_features_formats.html</a>. [Accessed: 13-Jan-2017]</p>
</div>
<div id="ref-_room_2017">
<p>[2] “Room EQ Wizard Room Acoustics Software.” 2017 [Online]. Available: <a href="https://www.roomeqwizard.com/" class="uri">https://www.roomeqwizard.com/</a>. [Accessed: 12-Jan-2017]</p>
</div>
<div id="ref-_operatic_2017">
<p>[3] “Operatic Voice The Open Acoustic Impulse Response Library.” 2017 [Online]. Available: <a href="http://www.openairlib.net/anechoicdb/content/operatic-voice" class="uri">http://www.openairlib.net/anechoicdb/content/operatic-voice</a>. [Accessed: 16-Jan-2017]</p>
</div>
<div id="ref-_reverberation_2017">
<p>[4] “Reverberation Time RT60.” 2017 [Online]. Available: <a href="http://www.nti-audio.com/en/functions/reverberation-time-rt60.aspx" class="uri">http://www.nti-audio.com/en/functions/reverberation-time-rt60.aspx</a>. [Accessed: 12-Jan-2017]</p>
</div>
<div id="ref-kuttruff_room_2009">
<p>[5] H. Kuttruff, <em>Room Acoustics, Fifth Edition</em>. CRC Press, 2009. </p>
</div>
<div id="ref-hodgson_when_1994">
<p>[6] M. Hodgson, “When is diffuse-field theory accurate?” <em>Canadian Acoustics</em>, vol. 22, no. 3, pp. 41–42, 1994. </p>
</div>
<div id="ref-hacihabiboglu_simulation_2010">
<p>[7] H. Hacıhabiboglu, B. Günel, and Z. Cvetkovic, “Simulation of directional microphones in digital waveguide mesh-based models of room acoustics,” <em>IEEE Trans. on Audio, Speech and Language Process</em>, vol. 18, no. 2, pp. 213–223, 2010. </p>
</div>
</div>

        <nav id="prev_next_nav">
    
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
            
            
                <a href="/wayverb/boundary.html" class="prev_page">Boundary modelling</a>
            

            
            
            
                <a href="/wayverb/demos.html" class="next_page">Demos</a>
            
        
    
        
    
        
    
</nav>

    </div>
    <footer id="footer" class="wrapper alt">
    <div class="inner">
        <ul class="menu">
			<li>
                &copy; Reuben Thomas 2016. All rights reserved.
            </li>
            <li>
                Design: <a href="http://html5up.net">HTML5 UP</a>, modified by Reuben Thomas.
            </li>
		</ul>
	</div>
</footer>

<!-- Scripts -->
<script src="/wayverb/assets/js/jquery.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrollex.min.js"></script>
<script src="/wayverb/assets/js/jquery.scrolly.min.js"></script>
<script src="/wayverb/assets/js/skel.min.js"></script>
<script src="/wayverb/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="/wayverb/assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/wayverb/assets/js/main.js"></script>

</div>
</body>
</html>
